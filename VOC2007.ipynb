{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tfrecode로 바꿔보자.\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import xml.etree.ElementTree as ET\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'dataset_name', 'pascalvoc',\n",
    "    'The name of the dataset to convert.')\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'dataset_dir', './data/VOCdevkit/VOC2007/',\n",
    "    'Directory where the original dataset is stored.')\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'output_name', 'pascalvoc',fo\n",
    "    'Basename used for TFRecords output files.')\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'output_dir', './',\n",
    "    'Output directory where to store TFRecords files.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOC_LABELS = {\n",
    "    'none': (0, 'Background'),\n",
    "    'aeroplane': (1, 'Vehicle'),\n",
    "    'bicycle': (2, 'Vehicle'),\n",
    "    'bird': (3, 'Animal'),\n",
    "    'boat': (4, 'Vehicle'),\n",
    "    'bottle': (5, 'Indoor'),\n",
    "    'bus': (6, 'Vehicle'),\n",
    "    'car': (7, 'Vehicle'),\n",
    "    'cat': (8, 'Animal'),\n",
    "    'chair': (9, 'Indoor'),\n",
    "    'cow': (10, 'Animal'),\n",
    "    'diningtable': (11, 'Indoor'),\n",
    "    'dog': (12, 'Animal'),\n",
    "    'horse': (13, 'Animal'),\n",
    "    'motorbike': (14, 'Vehicle'),\n",
    "    'person': (15, 'Person'),\n",
    "    'pottedplant': (16, 'Indoor'),\n",
    "    'sheep': (17, 'Animal'),\n",
    "    'sofa': (18, 'Indoor'),\n",
    "    'train': (19, 'Vehicle'),\n",
    "    'tvmonitor': (20, 'Indoor'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def int64_feature(value):\n",
    "    \"\"\"Wrapper for inserting int64 features into Example proto.\n",
    "    \"\"\"\n",
    "    if not isinstance(value, list):\n",
    "        value = [value]\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "\n",
    "def float_feature(value):\n",
    "    \"\"\"Wrapper for inserting float features into Example proto.\n",
    "    \"\"\"\n",
    "    if not isinstance(value, list):\n",
    "        value = [value]\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "\n",
    "def bytes_feature(value):\n",
    "    \"\"\"Wrapper for inserting bytes features into Example proto.\n",
    "    \"\"\"\n",
    "    if not isinstance(value, list):\n",
    "        value = [value]\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Original dataset organisation.\n",
    "DIRECTORY_ANNOTATIONS = 'Annotations/'\n",
    "DIRECTORY_IMAGES = 'JPEGImages/'\n",
    "\n",
    "# TFRecords convertion parameters.\n",
    "RANDOM_SEED = datetime.date.today().year + datetime.date.today().month + datetime.date.today().day \n",
    "SAMPLES_PER_FILES = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _process_image(directory, name): # 4번 - 디렉토리와 이미지네임을 가져다가 annotation xml 파일로부터 각종 정보를 리턴 \n",
    "    \"\"\"Process a image and annotation file.\n",
    "    Args:\n",
    "      filename: string, path to an image file e.g., '/path/to/example.JPG'.\n",
    "      coder: instance of ImageCoder to provide TensorFlow image coding utils.\n",
    "    Returns:\n",
    "      image_buffer: string, JPEG encoding of RGB image.\n",
    "      height: integer, image height in pixels.\n",
    "      width: integer, image width in pixels.\n",
    "    \"\"\"\n",
    "    # Read the image file.\n",
    "    filename = directory + DIRECTORY_IMAGES + name + '.jpg' #파일네임 \n",
    "    image_data = tf.gfile.FastGFile(filename, 'r').read() #리드 \n",
    "\n",
    "    # Read the XML annotation file.\n",
    "    filename = os.path.join(directory, DIRECTORY_ANNOTATIONS, name + '.xml') #엑스엠엘\n",
    "    tree = ET.parse(filename) #파싱준비\n",
    "    root = tree.getroot() #루트얻어옴 \n",
    "\n",
    "    # Image shape.\n",
    "    size = root.find('size')\n",
    "    shape = [int(size.find('height').text),\n",
    "             int(size.find('width').text),\n",
    "             int(size.find('depth').text)]\n",
    "    \n",
    "    # Find annotations.\n",
    "    bboxes = []\n",
    "    labels = []\n",
    "    labels_text = []\n",
    "    difficult = []\n",
    "    truncated = []\n",
    "    for obj in root.findall('object'): #object 다 찾아서 넣음 바운딩박스/라벨(번호)/라벨텍스트(이름)/voc에 있는 특징? 어려운거?/좀짤린거\n",
    "        label = obj.find('name').text\n",
    "        labels.append(int(VOC_LABELS[label][0]))\n",
    "        labels_text.append(label.encode('ascii'))\n",
    "\n",
    "        if obj.find('difficult'):\n",
    "            difficult.append(int(obj.find('difficult').text))\n",
    "        else:\n",
    "            difficult.append(0)\n",
    "        if obj.find('truncated'):\n",
    "            truncated.append(int(obj.find('truncated').text))\n",
    "        else:\n",
    "            truncated.append(0)\n",
    "\n",
    "        bbox = obj.find('bndbox')\n",
    "        bboxes.append((float(bbox.find('ymin').text) / shape[0],\n",
    "                       float(bbox.find('xmin').text) / shape[1],\n",
    "                       float(bbox.find('ymax').text) / shape[0],\n",
    "                       float(bbox.find('xmax').text) / shape[1]\n",
    "                       ))\n",
    "    return image_data, shape, bboxes, labels, labels_text, difficult, truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _convert_to_example(image_data, labels, labels_text, bboxes, shape,\n",
    "                        difficult, truncated):\n",
    "    \"\"\"Build an Example proto for an image example.\n",
    "    Args:\n",
    "      image_data: string, JPEG encoding of RGB image;\n",
    "      labels: list of integers, identifier for the ground truth;\n",
    "      labels_text: list of strings, human-readable labels;\n",
    "      bboxes: list of bounding boxes; each box is a list of integers;\n",
    "          specifying [xmin, ymin, xmax, ymax]. All boxes are assumed to belong\n",
    "          to the same label as the image label.\n",
    "      shape: 3 integers, image shapes in pixels.\n",
    "    Returns:\n",
    "      Example proto\n",
    "    \"\"\"\n",
    "    xmin = []\n",
    "    ymin = []\n",
    "    xmax = []\n",
    "    ymax = []\n",
    "    for b in bboxes:\n",
    "        assert len(b) == 4\n",
    "        # pylint: disable=expression-not-assigned\n",
    "        [l.append(point) for l, point in zip([ymin, xmin, ymax, xmax], b)] #l > coordinates list / b > boundingbox index\n",
    "        #2차원 배열([ymin, xmin, ymax, xmax])에 boundindx box 좌표를 순서대로 넣어주는 것 같다. \n",
    "        # ???? 잘 모르곘음 소수점이 막 나오는 리스트가 l이 되긴 하는데... \n",
    "        # pylint: enable=expression-not-assigned\n",
    "        \n",
    "    image_format = b'JPEG' #이미지 포멧 \n",
    "    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "            'image/height': int64_feature(shape[0]),\n",
    "            'image/width': int64_feature(shape[1]),\n",
    "            'image/channels': int64_feature(shape[2]),\n",
    "            'image/shape': int64_feature(shape),\n",
    "            'image/object/bbox/xmin': float_feature(xmin),\n",
    "            'image/object/bbox/xmax': float_feature(xmax),\n",
    "            'image/object/bbox/ymin': float_feature(ymin),\n",
    "            'image/object/bbox/ymax': float_feature(ymax),\n",
    "            'image/object/bbox/label': int64_feature(labels),\n",
    "            'image/object/bbox/label_text': bytes_feature(labels_text),\n",
    "            'image/object/bbox/difficult': int64_feature(difficult),\n",
    "            'image/object/bbox/truncated': int64_feature(truncated),\n",
    "            'image/format': bytes_feature(image_format), #포멧\n",
    "            'image/encoded': bytes_feature(image_data)})) #실데이터\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _add_to_tfrecord(dataset_dir, name, tfrecord_writer): #3번 - 데이터 불러와서 tf recode에 써줌 \n",
    "    \"\"\"Loads data from image and annotations files and add them to a TFRecord.\n",
    "    Args:\n",
    "      dataset_dir: Dataset directory;\n",
    "      name: Image name to add to the TFRecord;\n",
    "      tfrecord_writer: The TFRecord writer to use for writing.\n",
    "    \"\"\"\n",
    "    image_data, shape, bboxes, labels, labels_text, difficult, truncated = _process_image(dataset_dir, name) #이미지 어노테이션 데이터 \n",
    "    example = _convert_to_example(image_data, labels, labels_text, bboxes, shape, difficult, truncated) #받아온 데이터로 example proto만듬\n",
    "    tfrecord_writer.write(example.SerializeToString()) #시리얼라이즈 해서 써줌 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _get_output_filename(output_dir, name, idx): #2번 - output filename 만들어줌. \n",
    "    return '%s/%s_%03d.tfrecords' % (output_dir, name, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run(dataset_dir, output_dir, name='voc_train', shuffling=False): #1번 - 데이터 dir, 아웃풋 dir, 이름, 셔플여부 \n",
    "    \"\"\"Runs the conversion operation.\n",
    "    Args:\n",
    "      dataset_dir: The dataset directory where the dataset is stored.\n",
    "      output_dir: Output directory.\n",
    "    \"\"\"\n",
    "    if not tf.gfile.Exists(dataset_dir):\n",
    "        print('data directory 없다. 없으면 못쓰지')\n",
    "        tf.gfile.MakeDirs(dataset_dir)\n",
    "\n",
    "    # Dataset filenames, and shuffling. > annotation xml file list\n",
    "    path = os.path.join(dataset_dir, DIRECTORY_ANNOTATIONS)\n",
    "    filenames = sorted(os.listdir(path))\n",
    "\n",
    "    if shuffling:\n",
    "        random.seed(RANDOM_SEED)\n",
    "        random.shuffle(filenames)\n",
    "\n",
    "    # Process dataset files.\n",
    "    i = 0\n",
    "    fidx = 0\n",
    "    while i < len(filenames): # all files\n",
    "        # Open new TFRecord file.\n",
    "        tf_filename = _get_output_filename(output_dir, name, fidx) #파일네임 받아옴. \n",
    "        with tf.python_io.TFRecordWriter(tf_filename) as tfrecord_writer: #tf recode writer \n",
    "            j = 0\n",
    "            while i < len(filenames) and j < SAMPLES_PER_FILES: #한 tfrecode에 SAMPLES_PER_FILES만큼 넣어줌. \n",
    "                sys.stdout.write('\\r>> Converting image %d/%d' % (i+1, len(filenames)))\n",
    "                sys.stdout.flush()\n",
    "\n",
    "                filename = filenames[i]\n",
    "                img_name = filename[:-4] #이미지네임 가지고 tfrecode 만들어줌\n",
    "                _add_to_tfrecord(dataset_dir, img_name, tfrecord_writer)\n",
    "                i += 1\n",
    "                j += 1\n",
    "            fidx += 1 #filename index\n",
    "\n",
    "    # Finally, write the labels file:\n",
    "    # labels_to_class_names = dict(zip(range(len(_CLASS_NAMES)), _CLASS_NAMES))\n",
    "    # dataset_utils.write_label_file(labels_to_class_names, dataset_dir)\n",
    "    print('\\nFinished converting the Pascal VOC dataset!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Converting image 5011/5011\n",
      "Finished converting the Pascal VOC dataset!\n"
     ]
    }
   ],
   "source": [
    "run(FLAGS.dataset_dir, FLAGS.output_dir, FLAGS.output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = sess.run(softmax_tensor, {'DecodeJpeg/contents:0': image_data})\n",
    "def decode_jpeg(self, image_data):\n",
    "    image = self._sess.run(self._decode_jpeg,\n",
    "                           feed_dict={self._decode_jpeg_data: image_data})\n",
    "    assert len(image.shape) == 3\n",
    "    assert image.shape[2] == 3\n",
    "    return image\n",
    "decode_jpeg_data = tf.placeholder(dtype=tf.string)\n",
    "decode_jpeg = tf.image.decode_jpeg(decode_jpeg_data, channels=3)\n",
    "\n",
    "#self._png_to_jpeg = tf.image.encode_jpeg(image, format='rgb', quality=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. 이미지 GFast - distortion 포기해야 함. shuffle batch도 아직 어려움 + image height,width 알아야 함\n",
    "##### 2. 이미지 그대로 uint8로 읽어오기 - 속도가 조금 느려짐. distortion, shuffle 다 가능 \n",
    "##### 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/tensorflow/models/blob/master/inception/inception/data/build_image_data.py\n",
    "https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/examples/how_tos/reading_data/fully_connected_reader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "H, W, B = 24, 78, 9\n",
    "anchor_shapes = np.reshape(\n",
    "  [np.array(\n",
    "      [[  36.,  37.], [ 366., 174.], [ 115.,  59.],\n",
    "       [ 162.,  87.], [  38.,  90.], [ 258., 173.],\n",
    "       [ 224., 108.], [  78., 170.], [  72.,  43.]])] * H * W,\n",
    "  (H, W, B, 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 78, 9, 2)\n"
     ]
    }
   ],
   "source": [
    "print(anchor_shapes.shape)\n",
    "\n",
    "IMAGE_WIDTH           = 1242\n",
    "IMAGE_HEIGHT          = 375"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "center_x = np.reshape(\n",
    "  np.transpose(\n",
    "      np.reshape(\n",
    "          np.array([np.arange(1, W+1)*float(IMAGE_WIDTH)/(W+1)]*H*B), \n",
    "          (B, H, W)\n",
    "      ),\n",
    "      (1, 2, 0)\n",
    "  ),\n",
    "  (H, W, B, 1)\n",
    ")\n",
    "center_y = np.reshape(\n",
    "  np.transpose(\n",
    "      np.reshape(\n",
    "          np.array([np.arange(1, H+1)*float(IMAGE_HEIGHT)/(H+1)]*W*B),\n",
    "          (B, W, H)\n",
    "      ),\n",
    "      (2, 1, 0)\n",
    "  ),\n",
    "  (H, W, B, 1)\n",
    ")\n",
    "anchors = np.reshape(\n",
    "  np.concatenate((center_x, center_y, anchor_shapes), axis=3),\n",
    "  (-1, 4)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16848"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(anchors[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "line = \"a d w e g s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a d w e g s'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a d w e g s'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'd', 'w', 'e', 'g', 's']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line.split(' ' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'd', 'w', 'e', 'g', 's']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line.strip().split(' ' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dets_per_typ = {} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dets_per_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dets_per_typ['z'] = [{'1':'11'}]\n",
    "dets_per_typ['z'].append({'22':'222'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b': [{'1': '11'}, {'22': '222'}], 'z': [{'1': '11'}, {'22': '222'}]}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dets_per_typ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\n",
      "[{'1': '11'}, {'22': '222'}]\n",
      "z\n",
      "[{'1': '11'}, {'22': '222'}]\n"
     ]
    }
   ],
   "source": [
    "for error_type, dets in dets_per_typ.items():\n",
    "    print(error_type)\n",
    "    print(dets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
