{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORT\n",
      "1.0.1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "print(\"IMPORT\")\n",
    "import tensorflow as tf #tensorflow\n",
    "print(tf.__version__)\n",
    "import numpy as np #numpy > save loss .. \n",
    "from collections import OrderedDict #layer ..\n",
    "import os, random #dir, random..\n",
    "import pickle #save & load\n",
    "\n",
    "# cifar 10 \n",
    "#wget http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
    "#tar xzf cifar-10-python.tar.gz\n",
    "\n",
    "#resnet\n",
    "#https://arxiv.org/abs/1512.03385"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VARIABLES\n",
      "input image size : 1024\n",
      "image channel : 3\n",
      "batch size : 64\n",
      "num of class : 10\n",
      "training epochs : 1000\n",
      "learning rate : 0.01\n",
      "learning decay rate : 0.5\n"
     ]
    }
   ],
   "source": [
    "print(\"VARIABLES\")\n",
    "#path\n",
    "DATA_PATH = 'cifar10/cifar-10-batches-py/'\n",
    "CHECKPOINT_PATH = 'checkpoint/simple_res_net/simple_res_net.ckpt'\n",
    "SAVE_PATH = 'checkpoint/simple_res_net/'\n",
    "\n",
    "#parameters\n",
    "INPUT_SIDE = 32\n",
    "INPUT_SIZE = INPUT_SIDE * INPUT_SIDE\n",
    "N_CHANNEL = 3\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 1000\n",
    "LR = 0.01\n",
    "LR_DECAY_RATE = 0.5\n",
    "\n",
    "LIST_CLASS=['airplane', 'automobile', 'birds', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "N_CLASSES = len(LIST_CLASS)\n",
    "\n",
    "#train&test batch\n",
    "train_file=['data_batch_1','data_batch_2', 'data_batch_3', 'data_batch_4', 'data_batch_5']\n",
    "test_file=['cifar10/cifar-10-batches-py/test_batch']\n",
    "\n",
    "#input & output \n",
    "x = tf.placeholder(\"float\", [BATCH_SIZE, INPUT_SIZE * N_CHANNEL]) #batch x image size\n",
    "y = tf.placeholder(\"float\", [BATCH_SIZE, N_CLASSES]) #batch x class`\n",
    "\n",
    "\n",
    "print(\"input image size : {}\".format(INPUT_SIZE))\n",
    "print(\"image channel : {}\".format(N_CHANNEL))\n",
    "print(\"batch size : {}\".format(BATCH_SIZE))\n",
    "print(\"num of class : {}\".format(N_CLASSES))\n",
    "print(\"training epochs : {}\".format(EPOCHS))\n",
    "print(\"learning rate : {}\".format(LR))\n",
    "print(\"learning decay rate : {}\".format(LR_DECAY_RATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NETWORK PARAMETERS\n"
     ]
    }
   ],
   "source": [
    "#NETWORK PARAMETERS\n",
    "\n",
    "print(\"NETWORK PARAMETERS\")\n",
    "\n",
    "stddev = 0.1\n",
    "\n",
    "weights = {\n",
    "     'conv' : tf.Variable(tf.random_normal([3, 3, N_CHANNEL, 16], stddev=stddev)),\n",
    "    'conv1_1x1' : tf.Variable(tf.random_normal([1, 1, 16, 4], stddev=stddev)),\n",
    "    'conv1_3x3' : tf.Variable(tf.random_normal([3, 3, 4, 4], stddev=stddev)),\n",
    "    'conv1_1x1_16' : tf.Variable(tf.random_normal([1, 1, 4, 16], stddev=stddev)),\n",
    "    'conv2_1x1' : tf.Variable(tf.random_normal([1, 1, 16, 4], stddev=stddev)),\n",
    "    'conv2_3x3' : tf.Variable(tf.random_normal([3, 3, 4, 4], stddev=stddev)),\n",
    "    'conv2_1x1_16' : tf.Variable(tf.random_normal([1, 1, 4, 16], stddev=stddev)),\n",
    "    \n",
    "    # conv 16 + conv2 16 = 32filters \n",
    "    \n",
    "    'conv3_1x1' : tf.Variable(tf.random_normal([1, 1, 32, 8], stddev=stddev)),\n",
    "    'conv3_3x3' : tf.Variable(tf.random_normal([3, 3, 8, 8], stddev=stddev)),\n",
    "    'conv3_1x1_32' : tf.Variable(tf.random_normal([1, 1, 8, 32], stddev=stddev)),\n",
    "    'conv4_1x1' : tf.Variable(tf.random_normal([1, 1, 32, 8], stddev=stddev)),\n",
    "    'conv4_3x3' : tf.Variable(tf.random_normal([3, 3, 8, 8], stddev=stddev)),\n",
    "    'conv4_1x1_32' : tf.Variable(tf.random_normal([1, 1, 8, 32], stddev=stddev)),\n",
    "    \n",
    "    # conv2 지난거32 + conv4 32 = 64\n",
    "    \n",
    "    'conv5_1x1' : tf.Variable(tf.random_normal([1, 1, 64, 16], stddev=stddev)),\n",
    "    'conv5_3x3' : tf.Variable(tf.random_normal([3, 3, 16, 16], stddev=stddev)),\n",
    "    'conv5_1x1_64' : tf.Variable(tf.random_normal([1, 1, 16, 64], stddev=stddev)),\n",
    "    'conv6_1x1' : tf.Variable(tf.random_normal([1, 1, 64, 16], stddev=stddev)),\n",
    "    'conv6_3x3' : tf.Variable(tf.random_normal([3, 3, 16, 16], stddev=stddev)),\n",
    "    'conv6_1x1_64' : tf.Variable(tf.random_normal([1, 1, 16, 64], stddev=stddev)),\n",
    "    \n",
    "    #conv4 지난거 64 + conv6 64 = 128\n",
    "    \n",
    "    'dense1' : tf.Variable(tf.random_normal([16*16*128, 1000], stddev=stddev)),\n",
    "    'dense2' : tf.Variable(tf.random_normal([1000, N_CLASSES], stddev=stddev))\n",
    "    \n",
    "    \n",
    "}\n",
    "biases = {\n",
    "    'conv' : tf.Variable(tf.random_normal([16], stddev=stddev)),\n",
    "    'conv1_1x1' : tf.Variable(tf.random_normal([4], stddev=stddev)),\n",
    "    'conv1_3x3' : tf.Variable(tf.random_normal([4], stddev=stddev)),\n",
    "    'conv1_1x1_16' : tf.Variable(tf.random_normal([16], stddev=stddev)),\n",
    "    'conv2_1x1' : tf.Variable(tf.random_normal([4], stddev=stddev)),\n",
    "    'conv2_3x3' :tf.Variable(tf.random_normal([4], stddev=stddev)),\n",
    "    'conv2_1x1_16' : tf.Variable(tf.random_normal([16], stddev=stddev)),\n",
    "    \n",
    "    'conv3_1x1' : tf.Variable(tf.random_normal([8], stddev=stddev)),\n",
    "    'conv3_3x3' : tf.Variable(tf.random_normal([8], stddev=stddev)),\n",
    "    'conv3_1x1_32' : tf.Variable(tf.random_normal([32], stddev=stddev)),\n",
    "    'conv4_1x1' : tf.Variable(tf.random_normal([8], stddev=stddev)),\n",
    "    'conv4_3x3' : tf.Variable(tf.random_normal([8], stddev=stddev)),\n",
    "    'conv4_1x1_32' : tf.Variable(tf.random_normal([32], stddev=stddev)),\n",
    "    \n",
    "    'conv5_1x1' : tf.Variable(tf.random_normal([16], stddev=stddev)),\n",
    "    'conv5_3x3' : tf.Variable(tf.random_normal([16], stddev=stddev)),\n",
    "    'conv5_1x1_64' : tf.Variable(tf.random_normal([64], stddev=stddev)),\n",
    "    'conv6_1x1' : tf.Variable(tf.random_normal([16], stddev=stddev)),\n",
    "    'conv6_3x3' : tf.Variable(tf.random_normal([16], stddev=stddev)),\n",
    "    'conv6_1x1_64' : tf.Variable(tf.random_normal([64], stddev=stddev)),\n",
    "    \n",
    "    'dense1' : tf.Variable(tf.random_normal([1000], stddev=stddev)),\n",
    "    'dense2' : tf.Variable(tf.random_normal([N_CLASSES], stddev=stddev))\n",
    "    \n",
    "}\n",
    "\n",
    "w = {\n",
    "   \n",
    "    'conv' : tf.Variable(tf.random_normal([3, 3, N_CHANNEL, 8], stddev=stddev)),\n",
    "    'conv1_1x1' : tf.Variable(tf.random_normal([1, 1, 8, 2], stddev=stddev)),\n",
    "    'conv1_3x3' : tf.Variable(tf.random_normal([3, 3, 2, 2], stddev=stddev)),\n",
    "    'conv1_1x1_16' : tf.Variable(tf.random_normal([1, 1, 2, 8], stddev=stddev)),\n",
    "    'conv2_1x1' : tf.Variable(tf.random_normal([1, 1, 8, 2], stddev=stddev)),\n",
    "    'conv2_3x3' : tf.Variable(tf.random_normal([3, 3, 2, 2], stddev=stddev)),\n",
    "    'conv2_1x1_16' : tf.Variable(tf.random_normal([1, 1, 2, 8], stddev=stddev)),\n",
    "    \n",
    "    # conv 8 + conv2 8= 16filters \n",
    "    \n",
    "    'conv3_1x1' : tf.Variable(tf.random_normal([1, 1, 16, 4], stddev=stddev)),\n",
    "    'conv3_3x3' : tf.Variable(tf.random_normal([3, 3, 4, 4], stddev=stddev)),\n",
    "    'conv3_1x1_32' : tf.Variable(tf.random_normal([1, 1, 4, 16], stddev=stddev)),\n",
    "    'conv4_1x1' : tf.Variable(tf.random_normal([1, 1, 16, 4], stddev=stddev)),\n",
    "    'conv4_3x3' : tf.Variable(tf.random_normal([3, 3, 4, 4], stddev=stddev)),\n",
    "    'conv4_1x1_32' : tf.Variable(tf.random_normal([1, 1, 4, 16], stddev=stddev)),\n",
    "    \n",
    "    '''\n",
    "    # conv2 지난거16 + conv4 16 = 32\n",
    "    \n",
    "    'conv5_1x1' : tf.Variable(tf.random_normal([1, 1, 32, 8], stddev=stddev)),\n",
    "    'conv5_3x3' : tf.Variable(tf.random_normal([3, 3, 8, 8], stddev=stddev)),\n",
    "    'conv5_1x1_64' : tf.Variable(tf.random_normal([1, 1, 8, 32], stddev=stddev)),\n",
    "    'conv6_1x1' : tf.Variable(tf.random_normal([1, 1, 32, 8], stddev=stddev)),\n",
    "    'conv6_3x3' : tf.Variable(tf.random_normal([3, 3, 8, 8], stddev=stddev)),\n",
    "    'conv6_1x1_64' : tf.Variable(tf.random_normal([1, 1, 8, 32], stddev=stddev)),\n",
    "    \n",
    "    #conv4 지난거 32 + conv6 32 = 64\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    'dense1' : tf.Variable(tf.random_normal([16*16*32, 1000], stddev=stddev)),\n",
    "    'dense2' : tf.Variable(tf.random_normal([1000, N_CLASSES], stddev=stddev))\n",
    "}\n",
    "b = { \n",
    "  \n",
    "    'conv' : tf.Variable(tf.random_normal([8], stddev=stddev)),\n",
    "    'conv1_1x1' : tf.Variable(tf.random_normal([2], stddev=stddev)),\n",
    "    'conv1_3x3' : tf.Variable(tf.random_normal([2], stddev=stddev)),\n",
    "    'conv1_1x1_16' : tf.Variable(tf.random_normal([8], stddev=stddev)),\n",
    "    'conv2_1x1' : tf.Variable(tf.random_normal([2], stddev=stddev)),\n",
    "    'conv2_3x3' :tf.Variable(tf.random_normal([2], stddev=stddev)),\n",
    "    'conv2_1x1_16' : tf.Variable(tf.random_normal([8], stddev=stddev)),\n",
    "    \n",
    "    'conv3_1x1' : tf.Variable(tf.random_normal([4], stddev=stddev)),\n",
    "    'conv3_3x3' : tf.Variable(tf.random_normal([4], stddev=stddev)),\n",
    "    'conv3_1x1_32' : tf.Variable(tf.random_normal([16], stddev=stddev)),\n",
    "    'conv4_1x1' : tf.Variable(tf.random_normal([4], stddev=stddev)),\n",
    "    'conv4_3x3' : tf.Variable(tf.random_normal([4], stddev=stddev)),\n",
    "    'conv4_1x1_32' : tf.Variable(tf.random_normal([16], stddev=stddev)),\n",
    "    \n",
    "    'conv5_1x1' : tf.Variable(tf.random_normal([8], stddev=stddev)),\n",
    "    'conv5_3x3' : tf.Variable(tf.random_normal([8], stddev=stddev)),\n",
    "    'conv5_1x1_64' : tf.Variable(tf.random_normal([32], stddev=stddev)),\n",
    "    'conv6_1x1' : tf.Variable(tf.random_normal([8], stddev=stddev)),\n",
    "    'conv6_3x3' : tf.Variable(tf.random_normal([8], stddev=stddev)),\n",
    "    'conv6_1x1_64' : tf.Variable(tf.random_normal([32], stddev=stddev)),\n",
    "    \n",
    "    'dense1' : tf.Variable(tf.random_normal([1000], stddev=stddev)),\n",
    "    'dense2' : tf.Variable(tf.random_normal([N_CLASSES], stddev=stddev))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVER READY\n"
     ]
    }
   ],
   "source": [
    "#save\n",
    "saver = tf.train.Saver(max_to_keep=3)\n",
    "save_step = 100 #save for 100 epoch\n",
    "if not os.path.exists(CHECKPOINT_PATH):\n",
    "    os.makedirs(CHECKPOINT_PATH)\n",
    "print (\"SAVER READY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NETWORK FUNCTION\n"
     ]
    }
   ],
   "source": [
    "#model\n",
    "#http://laonple.blog.me/220764986252 - bottleneck \n",
    "#conv 3x3\n",
    "#conv (1x1, 3x3, 1x1) - relu > 16\n",
    "#conv (1x1, 3x3, 1x1) relu > 32\n",
    "#conv (1x1, 3x3, 1x1) relu > 64\n",
    "#avg pooling \n",
    "#fc\n",
    "#softmax\n",
    "\n",
    "print(\"NETWORK FUNCTION\")\n",
    "\n",
    "def SimpleResNet(img_width, img_height, img_channel, _x, _w, _b, scope='SimpleResNet'):\n",
    "    network = OrderedDict() #network layers\n",
    "\n",
    "    # X RESHAPE\n",
    "    _x_r = tf.reshape(_x, shape=[-1,img_width,img_height, img_channel])\n",
    "    \n",
    "    with tf.variable_scope(scope):\n",
    "        conv = tf.nn.conv2d(_x_r, _w['conv'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "        conv = tf.nn.bias_add(conv, _b['conv'])\n",
    "        conv = tf.nn.relu(conv)\n",
    "        network['conv'] = conv\n",
    "        \n",
    "        #16\n",
    "        conv1_1x1 = tf.nn.conv2d(conv, _w['conv1_1x1'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "        conv1_1x1 = tf.nn.bias_add(conv1_1x1, _b['conv1_1x1'])\n",
    "        conv1_1x1 = tf.nn.relu(conv1_1x1)\n",
    "        conv1_3x3 = tf.nn.conv2d(conv1_1x1, _w['conv1_3x3'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "        conv1_3x3 = tf.nn.bias_add(conv1_3x3, _b['conv1_3x3'])\n",
    "        conv1_3x3 = tf.nn.relu(conv1_3x3)\n",
    "        conv1_1x1_16 = tf.nn.conv2d(conv1_3x3, _w['conv1_1x1_16'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "        conv1_1x1_16 = tf.nn.bias_add(conv1_1x1_16, _b['conv1_1x1_16'])\n",
    "        conv1_1x1_16 = tf.nn.relu(conv1_1x1_16)\n",
    "        network['conv1_1x1_16'] = conv1_1x1_16\n",
    "        \n",
    "        #16\n",
    "        conv2_1x1 = tf.nn.conv2d(conv1_1x1_16, _w['conv2_1x1'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "        conv2_1x1 = tf.nn.bias_add(conv2_1x1, _b['conv2_1x1'])\n",
    "        conv2_1x1 = tf.nn.relu(conv2_1x1)\n",
    "        conv2_3x3 = tf.nn.conv2d(conv2_1x1, _w['conv2_3x3'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "        conv2_3x3 = tf.nn.bias_add(conv2_3x3, _b['conv2_3x3'])\n",
    "        conv2_3x3 = tf.nn.relu(conv2_3x3)\n",
    "        conv2_1x1_16 = tf.nn.conv2d(conv2_3x3, _w['conv2_1x1_16'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "        conv2_1x1_16 = tf.nn.bias_add(conv2_1x1_16, _b['conv2_1x1_16'])\n",
    "        #print(conv2_1x1_16.shape)\n",
    "        #32\n",
    "        conv2_1x1_16 = tf.concat([conv, conv2_1x1_16], 3) \n",
    "        conv2_1x1_16 = tf.nn.relu(conv2_1x1_16)\n",
    "        network['conv2_1x1_16'] = conv2_1x1_16\n",
    "       \n",
    "        #print(conv2_1x1_16.shape)\n",
    "        #32       32 x 32 x 16+16 >>> \n",
    "        conv3_1x1 = tf.nn.conv2d(conv2_1x1_16, _w['conv3_1x1'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "        conv3_1x1 = tf.nn.bias_add(conv3_1x1, _b['conv3_1x1'])\n",
    "        conv3_1x1 = tf.nn.relu(conv3_1x1)\n",
    "        conv3_3x3 = tf.nn.conv2d(conv3_1x1, _w['conv3_3x3'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "        conv3_3x3 = tf.nn.bias_add(conv3_3x3, _b['conv3_3x3'])\n",
    "        conv3_3x3 = tf.nn.relu(conv3_3x3)\n",
    "        conv3_1x1_32 = tf.nn.conv2d(conv3_3x3, _w['conv3_1x1_32'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "        conv3_1x1_32 = tf.nn.bias_add(conv3_1x1_32, _b['conv3_1x1_32'])\n",
    "        conv3_1x1_32 = tf.nn.relu(conv3_1x1_32)\n",
    "        network['conv3_1x1_32'] = conv3_1x1_32\n",
    "        \n",
    "        conv4_1x1 = tf.nn.conv2d(conv3_1x1_32, _w['conv4_1x1'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "        conv4_1x1 = tf.nn.bias_add(conv4_1x1, _b['conv4_1x1'])\n",
    "        conv4_1x1 = tf.nn.relu(conv4_1x1)\n",
    "        conv4_3x3 = tf.nn.conv2d(conv4_1x1, _w['conv4_3x3'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "        conv4_3x3 = tf.nn.bias_add(conv4_3x3, _b['conv4_3x3'])\n",
    "        conv4_3x3 = tf.nn.relu(conv4_3x3)\n",
    "        conv4_1x1_32 = tf.nn.conv2d(conv4_3x3, _w['conv4_1x1_32'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "        conv4_1x1_32 = tf.nn.bias_add(conv4_1x1_32, _b['conv4_1x1_32'])\n",
    "        \n",
    "        #64\n",
    "        conv4_1x1_32 = tf.concat([conv2_1x1_16, conv4_1x1_32 ], 3)\n",
    "        conv4_1x1_32 = tf.nn.relu(conv4_1x1_32)\n",
    "        network['conv4_1x1_32'] = conv4_1x1_32\n",
    "    \n",
    "        conv5_1x1 = tf.nn.conv2d(conv4_1x1_32, _w['conv5_1x1'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "        conv5_1x1 = tf.nn.bias_add(conv5_1x1, _b['conv5_1x1'])\n",
    "        conv5_1x1 = tf.nn.relu(conv5_1x1)\n",
    "        conv5_3x3 = tf.nn.conv2d(conv5_1x1, _w['conv5_3x3'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "        conv5_3x3 = tf.nn.bias_add(conv5_3x3, _b['conv5_3x3'])\n",
    "        conv5_3x3 = tf.nn.relu(conv5_3x3)\n",
    "        conv5_1x1_64 = tf.nn.conv2d(conv5_3x3, _w['conv5_1x1_64'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "        conv5_1x1_64 = tf.nn.bias_add(conv5_1x1_64, _b['conv5_1x1_64'])\n",
    "        conv5_1x1_64 = tf.nn.relu(conv5_1x1_64)\n",
    "        network['conv5_1x1_64'] = conv5_1x1_64\n",
    "        \n",
    "        conv6_1x1 = tf.nn.conv2d(conv5_1x1_64, _w['conv6_1x1'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "        conv6_1x1 = tf.nn.bias_add(conv6_1x1, _b['conv6_1x1'])\n",
    "        conv6_1x1 = tf.nn.relu(conv6_1x1)\n",
    "        conv6_3x3 = tf.nn.conv2d(conv6_1x1, _w['conv6_3x3'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "        conv6_3x3 = tf.nn.bias_add(conv6_3x3, _b['conv6_3x3'])\n",
    "        conv6_3x3 = tf.nn.relu(conv6_3x3)\n",
    "        conv6_1x1_64 = tf.nn.conv2d(conv6_3x3, _w['conv6_1x1_64'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "        conv6_1x1_64 = tf.nn.bias_add(conv6_1x1_64, _b['conv6_1x1_64'])\n",
    "        \n",
    "        #128\n",
    "        conv6_1x1_64 = tf.concat([conv4_1x1_32, conv6_1x1_64],3)\n",
    "        conv6_1x1_64 = tf.nn.relu(conv6_1x1_64)\n",
    "        network['conv6_1x1_64'] = conv6_1x1_64\n",
    "        \n",
    "        pool = tf.nn.avg_pool(conv4_1x1_32, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "        #pool = tf.nn.avg_pool(conv6_1x1_64, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "        network['pool'] = pool\n",
    "        \n",
    "        dense = tf.reshape(pool, [-1, _w['dense1'].get_shape().as_list()[0]])\n",
    "        dense1 = tf.add(tf.matmul(dense, _w['dense1']), _b['dense1'])\n",
    "        dense1 = tf.nn.relu(dense1)\n",
    "        network['dense1'] = dense1\n",
    "        \n",
    "        logit = tf.add(tf.matmul(dense1, _w['dense2']), _b['dense2'])\n",
    "        network['logit'] = logit\n",
    "        \n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHECK LAYERS\n",
      "conv Tensor(\"SimpleResNet_2/Relu:0\", shape=(64, 32, 32, 16), dtype=float32)\n",
      "conv1_1x1_16 Tensor(\"SimpleResNet_2/Relu_3:0\", shape=(64, 32, 32, 16), dtype=float32)\n",
      "conv2_1x1_16 Tensor(\"SimpleResNet_2/Relu_6:0\", shape=(64, 32, 32, 32), dtype=float32)\n",
      "conv3_1x1_32 Tensor(\"SimpleResNet_2/Relu_9:0\", shape=(64, 32, 32, 32), dtype=float32)\n",
      "conv4_1x1_32 Tensor(\"SimpleResNet_2/Relu_12:0\", shape=(64, 32, 32, 64), dtype=float32)\n",
      "conv5_1x1_64 Tensor(\"SimpleResNet_2/Relu_15:0\", shape=(64, 32, 32, 64), dtype=float32)\n",
      "conv6_1x1_64 Tensor(\"SimpleResNet_2/Relu_18:0\", shape=(64, 32, 32, 128), dtype=float32)\n",
      "pool Tensor(\"SimpleResNet_2/AvgPool:0\", shape=(64, 16, 16, 64), dtype=float32)\n",
      "dense1 Tensor(\"SimpleResNet_2/Relu_19:0\", shape=(32, 1000), dtype=float32)\n",
      "logit Tensor(\"SimpleResNet_2/Add_1:0\", shape=(32, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "out = SimpleResNet(INPUT_SIDE, INPUT_SIDE, N_CHANNEL, x, weights, biases, 'SimpleResNet')\n",
    "print(\"CHECK LAYERS\")\n",
    "for key, value in out.items():\n",
    "    print (key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimension 0 in both shapes must be equal, but are 32 and 64 for 'SoftmaxCrossEntropyWithLogits_1' (op: 'SoftmaxCrossEntropyWithLogits') with input shapes: [32,10], [64,10].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    670\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[1;32m    672\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Dimension 0 in both shapes must be equal, but are 32 and 64 for 'SoftmaxCrossEntropyWithLogits_1' (op: 'SoftmaxCrossEntropyWithLogits') with input shapes: [32,10], [64,10].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-3182246165d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'logit'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LOSS FUNCTION\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36msoftmax_cross_entropy_with_logits\u001b[0;34m(_sentinel, labels, logits, dim, name)\u001b[0m\n\u001b[1;32m   1615\u001b[0m   \u001b[0;31m# _CrossEntropyGrad() in nn_grad but not here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1616\u001b[0m   cost, unused_backprop = gen_nn_ops._softmax_cross_entropy_with_logits(\n\u001b[0;32m-> 1617\u001b[0;31m       precise_logits, labels, name=name)\n\u001b[0m\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m   \u001b[0;31m# The output cost shape should be the input minus dim.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36m_softmax_cross_entropy_with_logits\u001b[0;34m(features, labels, name)\u001b[0m\n\u001b[1;32m   2263\u001b[0m   \"\"\"\n\u001b[1;32m   2264\u001b[0m   result = _op_def_lib.apply_op(\"SoftmaxCrossEntropyWithLogits\",\n\u001b[0;32m-> 2265\u001b[0;31m                                 features=features, labels=labels, name=name)\n\u001b[0m\u001b[1;32m   2266\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0m_SoftmaxCrossEntropyWithLogitsOutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    761\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    762\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2327\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[1;32m   2328\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2329\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2330\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2331\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1715\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1717\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1718\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1719\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1666\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1667\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1669\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    608\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    609\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m                                   debug_python_shape_fn, require_shape_fn)\n\u001b[0m\u001b[1;32m    611\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    674\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimension 0 in both shapes must be equal, but are 32 and 64 for 'SoftmaxCrossEntropyWithLogits_1' (op: 'SoftmaxCrossEntropyWithLogits') with input shapes: [32,10], [64,10]."
     ]
    }
   ],
   "source": [
    "#cost\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=out['logit']))\n",
    "tf.summary.scalar(\"loss\", cost)\n",
    "print(\"LOSS FUNCTION\")\n",
    "\n",
    "#learning rate\n",
    "global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "learning_rate = tf.train.exponential_decay(learning_rate=LR,\n",
    "                                           global_step=global_step,\n",
    "                                           decay_steps=50000,\n",
    "                                           decay_rate=LR_DECAY_RATE,\n",
    "                                           staircase=True,\n",
    "                                           name=\"learning_rate\")\n",
    "learning_rate = tf.maximum(learning_rate, 0.0001)\n",
    "tf.summary.scalar(\"learning_rate\", learning_rate)\n",
    "print(\"LERANING RATE : {}\".format(learning_rate))\n",
    "\n",
    "#optimizer\n",
    "adam = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "sgd = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "print(\"OPTIMIZER 1 : {}\".format(adam))\n",
    "print(\"OPTIMIZER 2 : {}\".format(sgd))\n",
    "\n",
    "corr = tf.equal(tf.argmax(out['logit'], 1), tf.argmax(y,1))\n",
    "accr = tf.reduce_mean(tf.cast(corr, \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "\n",
    "#summary_writer = tf.summary.FileWriter(CHECKPOINT_PATH, sess.graph)\n",
    "#checkpoint = tf.train.latesrt_checkpoint(CHECKPOINT_PATH)\n",
    "\n",
    "#포문돌다 세이브할땐\n",
    "#save_path = saver.save(sess, CHECKPOINT_PATH + \"mm.ckpt\", global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#training\n",
    "for epoch in range(EPOCHS):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictor = prediction.Predictor(checkpoint_file)\n",
    "predictions, scores, bboxes = predictor.predict(os.path.join(image_dir, image_file_name), sliding_windows)\n",
    "predictions = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "queue_positives = tf.RandomShuffleQueue(10000, \n",
    "                                            min_after_dequeue=5000,\n",
    "                                            dtypes=[tf.float32, tf.int32], \n",
    "                                            shapes=[[PATCH_HEIGHT, PATCH_WIDTH, 1], []])\n",
    "    \n",
    "    queue_negatives = tf.RandomShuffleQueue(10000, \n",
    "                                            min_after_dequeue=5000,\n",
    "                                            dtypes=[tf.float32, tf.int32], \n",
    "                                            shapes=[[PATCH_HEIGHT, PATCH_WIDTH, 1], []])\n",
    "    \n",
    "    filename_placeholder = tf.placeholder(tf.string, name=\"filename\")    \n",
    "    labels_placeholder = tf.placeholder(tf.int32, shape=[None, ], name=\"labels\")\n",
    "    bboxes_placeholder = tf.placeholder(tf.int32, shape=[None, 4], name=\"bboxes_placeholder\")\n",
    "    \n",
    "    encoded = tf.read_file(filename_placeholder)\n",
    "    image = tf.image.decode_image(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_patch(bbox):        \n",
    "        patch = tf.image.crop_to_bounding_box(image, bbox[1], bbox[0], bbox[3], bbox[2])\n",
    "        patch = tf.image.per_image_standardization(patch)\n",
    "        patch = tf.image.resize_images(patch, [PATCH_HEIGHT, PATCH_WIDTH])\n",
    "        patch = tf.image.random_flip_left_right(patch)\n",
    "        patch = tf.image.random_flip_up_down(patch)    \n",
    "        patch = tf.image.random_brightness(patch, 0.15)\n",
    "        patch.set_shape([PATCH_HEIGHT, PATCH_WIDTH, 1])\n",
    "        return patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patches = tf.map_fn(prepare_patch, bboxes_placeholder, dtype=tf.float32)\n",
    "    negative_patches, positive_patches = tf.dynamic_partition(patches, labels_placeholder, 2)\n",
    "    negative_labels, positive_labels = tf.dynamic_partition(labels_placeholder, labels_placeholder, 2)\n",
    "    \n",
    "    enqueue_negatives = queue_negatives.enqueue_many([negative_patches, negative_labels])\n",
    "    enqueue_positives = queue_positives.enqueue_many([positive_patches, positive_labels])\n",
    "    enqueue_op = tf.group(enqueue_negatives, enqueue_positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negative_images, negative_labels = tf.train.batch([negative_image, negative_label], \n",
    "                                                      batch_size=128,\n",
    "                                                      num_threads=4,\n",
    "                                                      capacity=100,\n",
    "                                                      shapes=[[PATCH_HEIGHT, PATCH_WIDTH, 1], []])\n",
    "\n",
    "    positive_images, positive_labels = tf.train.batch([positive_image, positive_label], \n",
    "                                                      batch_size=128,\n",
    "                                                      num_threads=4,\n",
    "                                                      capacity=100,\n",
    "                                                      shapes=[[PATCH_HEIGHT, PATCH_WIDTH, 1], []])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits = network.network(negative_images, 2)\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=negative_labels, logits=logits)\n",
    "_, indices = tf.nn.top_k(losses, REGION_PER_IMAGE // 4, sorted=False)\n",
    "negative_images = tf.gather(negative_images, indices)\n",
    "negative_labels = tf.gather(negative_labels, indices)\n",
    "\n",
    "logits = network.network(positive_images, 2, reuse=True)\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=positive_labels, logits=logits)\n",
    "_, indices = tf.nn.top_k(losses, REGION_PER_IMAGE // 4, sorted=False)\n",
    "positive_images = tf.gather(positive_images, indices)\n",
    "positive_labels = tf.gather(positive_labels, indices)\n",
    "\n",
    "images = tf.concat([negative_images, positive_images], axis=0)\n",
    "labels = tf.concat([negative_labels, positive_labels], axis=0)\n",
    "\n",
    "# tf.summary.image(\"filtered_images\", images, max_outputs=REGION_PER_IMAGE // 2)\n",
    "\n",
    "logits = network.network(images, 2, is_training=True, reuse=True)\n",
    "tf.losses.sparse_softmax_cross_entropy(labels, logits)\n",
    "total_loss = tf.losses.get_total_loss()\n",
    "tf.summary.scalar(\"total_loss\", total_loss)\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "learning_rate = tf.train.exponential_decay(learning_rate=LR, \n",
    "                                           global_step=global_step, \n",
    "                                           decay_steps=50000,\n",
    "                                           decay_rate=LR_DECAY_RATE, \n",
    "                                           staircase=True,\n",
    "                                           name=\"learning_rate\")\n",
    "learning_rate = tf.maximum(learning_rate, 0.0001)\n",
    "tf.summary.scalar(\"learning_rate\", learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9, use_nesterov=True)\n",
    "train_op = tf.contrib.training.create_train_op(total_loss, optimizer, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def prepare_evaluations(filename):\n",
    "    encoded = tf.read_file(filename)\n",
    "    image = tf.image.decode_image(encoded)\n",
    "    image = tf.image.per_image_standardization(image)\n",
    "    image.set_shape([PATCH_HEIGHT, PATCH_WIDTH, 1])\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        feed_dict = {filename_placeholder: \"{}/{}\".format(TRAIN_DIR, filename),\n",
    "                     labels_placeholder: labels,\n",
    "                     bboxes_placeholder: bounding_boxes}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        sess.run(enqueue_op, feed_dict=feed_dict)        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def network(inputs, num_outputs, weight_decay=0.0005, is_training=False, reuse=False, scope=\"network\"):    \n",
    "    with tf.variable_scope(scope, values=[inputs], reuse=reuse):                  \n",
    "        # layer1\n",
    "        net = tf.layers.conv2d(inputs, 64, (5, 5), \n",
    "                               use_bias=False,                               \n",
    "                               kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(), \n",
    "                               kernel_regularizer=tf.contrib.layers.l2_regularizer(weight_decay), \n",
    "                               name=\"conv1\")\n",
    "        net = tf.layers.batch_normalization(net,                                                         \n",
    "                                            training=is_training, \n",
    "                                            name=\"batch_norm1\")\n",
    "        net = tf.nn.relu(net)\n",
    "        net = tf.layers.max_pooling2d(net, (2, 2), 2, name=\"pool1\")\n",
    "        \n",
    "        # layer2\n",
    "        net = tf.layers.conv2d(net, 128, (3, 3), \n",
    "                               use_bias=False,                                                                     \n",
    "                               kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(), \n",
    "                               kernel_regularizer=tf.contrib.layers.l2_regularizer(weight_decay), \n",
    "                               name=\"conv2\")\n",
    "        net = tf.layers.batch_normalization(net,             \n",
    "                                            training=is_training, \n",
    "                                            name=\"batch_norm2\")\n",
    "        net = tf.nn.relu(net)\n",
    "        net = tf.layers.max_pooling2d(net, (2, 2), 2, name=\"pool2\")\n",
    "        \n",
    "        # layer3\n",
    "        net = tf.layers.conv2d(net, 192, (3, 3), \n",
    "                               use_bias=False,        \n",
    "                               kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(), \n",
    "                               kernel_regularizer=tf.contrib.layers.l2_regularizer(weight_decay), \n",
    "                               name=\"conv3\")\n",
    "        net = tf.layers.batch_normalization(net,      \n",
    "                                            training=is_training, \n",
    "                                            name=\"batch_norm3\")\n",
    "        net = tf.nn.relu(net)\n",
    "        net = tf.layers.max_pooling2d(net, (2, 2), 2, name=\"pool3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " \n",
    "        # layer5\n",
    "        net = tf.layers.conv2d(net, 192, (3, 3), \n",
    "                               padding=\"same\",\n",
    "                               use_bias=False,        \n",
    "                               kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(), \n",
    "                               kernel_regularizer=tf.contrib.layers.l2_regularizer(weight_decay),\n",
    "                               name=\"conv5\")\n",
    "        net = tf.layers.batch_normalization(net,      \n",
    "                                            training=is_training, \n",
    "                                            name=\"batch_norm5\")\n",
    "        net = tf.nn.relu(net)\n",
    "        net = tf.layers.max_pooling2d(net, (2, 2), 2, name=\"pool5\")\n",
    "        net = tf.contrib.layers.flatten(net, scope=\"pool5_flatten\")\n",
    "        \n",
    "        # layer6\n",
    "        net = tf.layers.dense(net, 3072, \n",
    "                              use_bias=False, \n",
    "                              kernel_initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                              kernel_regularizer=tf.contrib.layers.l2_regularizer(weight_decay),\n",
    "                              name=\"fc6\")        \n",
    "        net = tf.layers.batch_normalization(net, \n",
    "                                            training=is_training, \n",
    "                                            name=\"batch_norm6\")        \n",
    "        net = tf.nn.relu(net)\n",
    "        \n",
    "        # layer7\n",
    "        net = tf.layers.dense(net, 1024, \n",
    "                              use_bias=False, \n",
    "                              kernel_initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                              kernel_regularizer=tf.contrib.layers.l2_regularizer(weight_decay),\n",
    "                              name=\"fc7\")        \n",
    "        net = tf.nn.relu(net)        \n",
    "        if is_training:\n",
    "            net = tf.nn.dropout(net, 0.5, name=\"dropout7\")\n",
    "\n",
    "        # layer8\n",
    "        return tf.layers.dense(net, num_outputs, \n",
    "                               use_bias=True, \n",
    "                               kernel_initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                               kernel_regularizer=tf.contrib.layers.l2_regularizer(weight_decay),\n",
    "                               bias_initializer=tf.zeros_initializer(),\n",
    "                               name=\"fc8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMAGE_WIDTH = 6576\n",
    "IMAGE_HEIGHT = 4384\n",
    "PATCH_WIDTH = 128\n",
    "PATCH_HEIGHT = 128\n",
    "REGION_PER_IMAGE = 128\n",
    "LR = 0.01\n",
    "LR_DECAY_RATE = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " logits = networks(images, 2, is_training=True, reuse=True)\n",
    "        tf.losses.sparse_softmax_cross_entropy(labels, logits)\n",
    "        total_loss = tf.losses.get_total_loss()\n",
    "        tf.summary.scalar(\"total_loss\", total_loss)\n",
    "\n",
    "        global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "        learning_rate = tf.train.exponential_decay(learning_rate=LR,\n",
    "                                                   global_step=global_step,\n",
    "                                                   decay_steps=50000,\n",
    "                                                   decay_rate=LR_DECAY_RATE,\n",
    "                                                   staircase=True,\n",
    "                                                   name=\"learning_rate\")\n",
    "        learning_rate = tf.maximum(learning_rate, 0.0001)\n",
    "        tf.summary.scalar(\"learning_rate\", learning_rate)\n",
    "\n",
    "        optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9, use_nesterov=True)\n",
    "        train_op = tf.contrib.training.create_train_op(total_loss, optimizer, global_step=global_step)\n",
    "\n",
    "        def prepare_evaluations(filename):\n",
    "            encoded = tf.read_file(filename)\n",
    "            image = tf.image.decode_image(encoded)\n",
    "            image = tf.image.per_image_standardization(image)\n",
    "            image.set_shape([PATCH_HEIGHT, PATCH_WIDTH, 1])\n",
    "            return image\n",
    "\n",
    "        filenames_placeholder = tf.placeholder(tf.string, shape=[None, ], name=\"eval_filenames\")\n",
    "        images = tf.map_fn(prepare_evaluations, filenames_placeholder, dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-533b2ce24a6d>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-533b2ce24a6d>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    오버피팅 제거기술\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "어그맨테이션\n",
    "오버피팅 제거기술\n",
    "파인튜닝 마저 하기 \n",
    "텐서보드\n",
    "  miss classfication 된 아이디를 저장해둬볼것\n",
    "        주로 뭐가 가장 많이 미쓰나는지\n",
    "        분류를 잘못했는지 데이터가 부족한지 결론낼수잇으.\n",
    "\n",
    "        비쥬얼라이제이션\n",
    "        피쳐 중간에 나오는거 어떻게 나오는지 이런거 확인\n",
    "in queue 이거 해보기 \n",
    "\n",
    "\n",
    "\n",
    "checkpoint\n",
    "        학습기법\n",
    "        sgd\n",
    "\n",
    "        pool 제거\n",
    "        decay\n",
    "\n",
    "        인셉션 모듈 추가\n",
    "        채널 +\n",
    "\n",
    "        어그멘테이션\n",
    "\n",
    "        세이브\n",
    "\n",
    "        오버피팅 제외하는거 드랍아웃이나 배치놈등등\n",
    "        배치 작게\n",
    "        64개\n",
    "        32개정도 960\n",
    "\n",
    "        dense레이어 갯수\n",
    "\n",
    "        파인튜닝 - 불러와서 재학습\n",
    "\n",
    "        모두연 레스넷 자료\n",
    "\n",
    "        마지막에 에버리지풀링 넣기\n",
    "        sparse -> dense 효과\n",
    "        그래프\n",
    "        loss같은거 보드에 넣어보기\n",
    "\n",
    "        miss classfication 된 아이디를 저장해둬볼것\n",
    "        주로 뭐가 가장 많이 미쓰나는지\n",
    "        분류를 잘못했는지 데이터가 부족한지 결론낼수잇으.\n",
    "\n",
    "        비쥬얼라이제이션\n",
    "        피쳐 중간에 나오는거 어떻게 나오는지 이런거 확인\n",
    "\n",
    "        1. 다시짜기 - 레이어 세이브 roc test\n",
    "        2. 아규멘테이션 - 플립 로테 크롭\n",
    "        3. 비쥬얼라이제이션 & 미스 클래시피케이션 데이터 추출 > 텐서보드 없이\n",
    "        4. queue\n",
    "        5. weight 업데이트 디케이, 스텝, 아담+ㅇ스지디\n",
    "\n",
    "        추가로 되면 레이어추간데 벅참"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "ll = []\n",
    "print(ll)\n",
    "ll.append(1)\n",
    "\n",
    "print(ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
