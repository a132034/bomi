{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "print(\"IMPORT\")\n",
    "import tensorflow as tf #tensorflow\n",
    "print(tf.__version__)\n",
    "import numpy as np #numpy > save loss .. \n",
    "from collections import OrderedDict #layer ..\n",
    "import os, random #dir, random..\n",
    "import pickle #save & load\n",
    "\n",
    "# cifar 10 \n",
    "#wget http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
    "#tar xzf cifar-10-python.tar.gz\n",
    "\n",
    "#resnet\n",
    "#https://arxiv.org/abs/1512.03385"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sparse_softmax_cross_entropy_with_logitsprint(\"VARIABLES\")\n",
    "#path\n",
    "DATA_PATH = 'cifar10/cifar-10-batches-py/'\n",
    "CHECKPOINT_PATH = 'checkpoint/simple_res_net/simple_res_net.ckpt'\n",
    "SAVE_PATH = 'checkpoint/simple_res_net/'\n",
    "\n",
    "#parameters\n",
    "INPUT_SIDE = 32\n",
    "INPUT_SIZE = INPUT_SIDE * INPUT_SIDE\n",
    "N_CHANNEL = 3\n",
    "BATCH_SIZE = 10\n",
    "EPOCHS = 1000\n",
    "LR = 0.01\n",
    "LR_DECAY_RATE = 0.5\n",
    "\n",
    "LIST_CLASS=['airplane', 'automobile', 'birds', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "N_CLASSES = len(LIST_CLASS)\n",
    "\n",
    "#train&test batch\n",
    "train_file=['data_batch_1','data_batch_2', 'data_batch_3', 'data_batch_4', 'data_batch_5']\n",
    "test_file=['cifar10/cifar-10-batches-py/test_batch']\n",
    "\n",
    "#input & output \n",
    "x = tf.placeholder(\"float\", [BATCH_SIZE, INPUT_SIZE * N_CHANNEL]) #batch x image size\n",
    "y = tf.placeholder(\"float\", [BATCH_SIZE, N_CLASSES]) #batch x class`\n",
    "\n",
    "\n",
    "print(\"input image size : {}\".format(INPUT_SIZE))\n",
    "print(\"image channel : {}\".format(N_CHANNEL))\n",
    "print(\"batch size : {}\".format(BATCH_SIZE))\n",
    "print(\"num of class : {}\".format(N_CLASSES))\n",
    "print(\"training epochs : {}\".format(EPOCHS))\n",
    "print(\"learning rate : {}\".format(LR))\n",
    "print(\"learning decay rate : {}\".format(LR_DECAY_RATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NETWORK PARAMETERS\n",
    "\n",
    "print(\"NETWORK PARAMETERS\")\n",
    "\n",
    "stddev = 0.1\n",
    "\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    weights = {\n",
    "        'conv' : tf.Variable(tf.random_normal([3, 3, N_CHANNEL, 8], stddev=stddev)),\n",
    "        'conv1_1x1' : tf.Variable(tf.random_normal([1, 1, 8, 2], stddev=stddev)),\n",
    "        'conv1_3x3' : tf.Variable(tf.random_normal([3, 3, 2, 2], stddev=stddev)),\n",
    "        'conv1_1x1_16' : tf.Variable(tf.random_normal([1, 1, 2, 8], stddev=stddev)),\n",
    "        'conv2_1x1' : tf.Variable(tf.random_normal([1, 1, 8, 2], stddev=stddev)),\n",
    "        'conv2_3x3' : tf.Variable(tf.random_normal([3, 3, 2, 2], stddev=stddev)),\n",
    "        'conv2_1x1_16' : tf.Variable(tf.random_normal([1, 1, 2, 8], stddev=stddev)),\n",
    "\n",
    "        # conv 8 + conv2 8= 16filters \n",
    "\n",
    "        'conv3_1x1' : tf.Variable(tf.random_normal([1, 1, 16, 4], stddev=stddev)),\n",
    "        'conv3_3x3' : tf.Variable(tf.random_normal([3, 3, 4, 4], stddev=stddev)),\n",
    "        'conv3_1x1_32' : tf.Variable(tf.random_normal([1, 1, 4, 16], stddev=stddev)),\n",
    "        'conv4_1x1' : tf.Variable(tf.random_normal([1, 1, 16, 4], stddev=stddev)),\n",
    "        'conv4_3x3' : tf.Variable(tf.random_normal([3, 3, 4, 4], stddev=stddev)),\n",
    "        'conv4_1x1_32' : tf.Variable(tf.random_normal([1, 1, 4, 16], stddev=stddev)),\n",
    "\n",
    "        # conv2 지난거 16 + conv4 16 = 32 filters\n",
    "\n",
    "        'dense1' : tf.Variable(tf.random_normal([16*16*32, 1000], stddev=stddev, name='dense11')),\n",
    "        'dense2' : tf.Variable(tf.random_normal([1000, N_CLASSES], stddev=stddev, name='dense22'))\n",
    "\n",
    "\n",
    "    }\n",
    "    biases = {\n",
    "\n",
    "        'conv' : tf.Variable(tf.random_normal([8], stddev=stddev)),\n",
    "        'conv1_1x1' : tf.Variable(tf.random_normal([2], stddev=stddev)),\n",
    "        'conv1_3x3' : tf.Variable(tf.random_normal([2], stddev=stddev)),\n",
    "        'conv1_1x1_16' : tf.Variable(tf.random_normal([8], stddev=stddev)),\n",
    "        'conv2_1x1' : tf.Variable(tf.random_normal([2], stddev=stddev)),\n",
    "        'conv2_3x3' :tf.Variable(tf.random_normal([2], stddev=stddev)),\n",
    "        'conv2_1x1_16' : tf.Variable(tf.random_normal([8], stddev=stddev)),\n",
    "\n",
    "        'conv3_1x1' : tf.Variable(tf.random_normal([4], stddev=stddev)),\n",
    "        'conv3_3x3' : tf.Variable(tf.random_normal([4], stddev=stddev)),\n",
    "        'conv3_1x1_32' : tf.Variable(tf.random_normal([16], stddev=stddev)),\n",
    "        'conv4_1x1' : tf.Variable(tf.random_normal([4], stddev=stddev)),\n",
    "        'conv4_3x3' : tf.Variable(tf.random_normal([4], stddev=stddev)),\n",
    "        'conv4_1x1_32' : tf.Variable(tf.random_normal([16], stddev=stddev)),\n",
    "\n",
    "        'dense1' : tf.Variable(tf.random_normal([1000], stddev=stddev)),\n",
    "        'dense2' : tf.Variable(tf.random_normal([N_CLASSES], stddev=stddev))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model\n",
    "#http://laonple.blog.me/220764986252 - bottleneck \n",
    "#conv 3x3\n",
    "#conv (1x1, 3x3, 1x1) - relu > 16\n",
    "#conv (1x1, 3x3, 1x1) relu > 32\n",
    "#conv (1x1, 3x3, 1x1) relu > 64\n",
    "#avg pooling \n",
    "#fc\n",
    "#softmax\n",
    "\n",
    "print(\"NETWORK FUNCTION\")\n",
    "\n",
    "def SimpleResNet(img_width, img_height, img_channel, _x, _w, _b, scope='SimpleResNet'):\n",
    "    network = OrderedDict() #network layers\n",
    "\n",
    "    # X RESHAPE\n",
    "    _x_r = tf.reshape(_x, shape=[-1,img_width,img_height, img_channel])\n",
    "    \n",
    "    with tf.variable_scope(scope):\n",
    "        conv = tf.nn.conv2d(_x_r, _w['conv'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "        conv = tf.nn.bias_add(conv, _b['conv'])\n",
    "        conv = tf.nn.relu(conv)\n",
    "        network['conv'] = conv\n",
    "        \n",
    "        #16\n",
    "        conv1_1x1 = tf.nn.conv2d(conv, _w['conv1_1x1'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "        conv1_1x1 = tf.nn.bias_add(conv1_1x1, _b['conv1_1x1'])\n",
    "        conv1_1x1 = tf.nn.relu(conv1_1x1)\n",
    "        conv1_3x3 = tf.nn.conv2d(conv1_1x1, _w['conv1_3x3'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "        conv1_3x3 = tf.nn.bias_add(conv1_3x3, _b['conv1_3x3'])\n",
    "        conv1_3x3 = tf.nn.relu(conv1_3x3)\n",
    "        conv1_1x1_16 = tf.nn.conv2d(conv1_3x3, _w['conv1_1x1_16'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "        conv1_1x1_16 = tf.nn.bias_add(conv1_1x1_16, _b['conv1_1x1_16'])\n",
    "        conv1_1x1_16 = tf.nn.relu(conv1_1x1_16)\n",
    "        network['conv1_1x1_16'] = conv1_1x1_16\n",
    "        \n",
    "        #16\n",
    "        conv2_1x1 = tf.nn.conv2d(conv1_1x1_16, _w['conv2_1x1'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "        conv2_1x1 = tf.nn.bias_add(conv2_1x1, _b['conv2_1x1'])\n",
    "        conv2_1x1 = tf.nn.relu(conv2_1x1)\n",
    "        conv2_3x3 = tf.nn.conv2d(conv2_1x1, _w['conv2_3x3'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "        conv2_3x3 = tf.nn.bias_add(conv2_3x3, _b['conv2_3x3'])\n",
    "        conv2_3x3 = tf.nn.relu(conv2_3x3)\n",
    "        conv2_1x1_16 = tf.nn.conv2d(conv2_3x3, _w['conv2_1x1_16'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "        conv2_1x1_16 = tf.nn.bias_add(conv2_1x1_16, _b['conv2_1x1_16'])\n",
    "        #print(conv2_1x1_16.shape)\n",
    "        #32\n",
    "        conv2_1x1_16 = tf.concat([conv, conv2_1x1_16], 3) \n",
    "        conv2_1x1_16 = tf.nn.relu(conv2_1x1_16)\n",
    "        network['conv2_1x1_16'] = conv2_1x1_16\n",
    "       \n",
    "        #print(conv2_1x1_16.shape)\n",
    "        #32       32 x 32 x 16+16 >>> \n",
    "        conv3_1x1 = tf.nn.conv2d(conv2_1x1_16, _w['conv3_1x1'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "        conv3_1x1 = tf.nn.bias_add(conv3_1x1, _b['conv3_1x1'])\n",
    "        conv3_1x1 = tf.nn.relu(conv3_1x1)\n",
    "        conv3_3x3 = tf.nn.conv2d(conv3_1x1, _w['conv3_3x3'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "        conv3_3x3 = tf.nn.bias_add(conv3_3x3, _b['conv3_3x3'])\n",
    "        conv3_3x3 = tf.nn.relu(conv3_3x3)\n",
    "        conv3_1x1_32 = tf.nn.conv2d(conv3_3x3, _w['conv3_1x1_32'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "        conv3_1x1_32 = tf.nn.bias_add(conv3_1x1_32, _b['conv3_1x1_32'])\n",
    "        conv3_1x1_32 = tf.nn.relu(conv3_1x1_32)\n",
    "        network['conv3_1x1_32'] = conv3_1x1_32\n",
    "        \n",
    "        conv4_1x1 = tf.nn.conv2d(conv3_1x1_32, _w['conv4_1x1'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "        conv4_1x1 = tf.nn.bias_add(conv4_1x1, _b['conv4_1x1'])\n",
    "        conv4_1x1 = tf.nn.relu(conv4_1x1)\n",
    "        conv4_3x3 = tf.nn.conv2d(conv4_1x1, _w['conv4_3x3'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "        conv4_3x3 = tf.nn.bias_add(conv4_3x3, _b['conv4_3x3'])\n",
    "        conv4_3x3 = tf.nn.relu(conv4_3x3)\n",
    "        conv4_1x1_32 = tf.nn.conv2d(conv4_3x3, _w['conv4_1x1_32'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "        conv4_1x1_32 = tf.nn.bias_add(conv4_1x1_32, _b['conv4_1x1_32'])\n",
    "        \n",
    "        #64\n",
    "        conv4_1x1_32 = tf.concat([conv2_1x1_16, conv4_1x1_32 ], 3)\n",
    "        conv4_1x1_32 = tf.nn.relu(conv4_1x1_32)\n",
    "        network['conv4_1x1_32'] = conv4_1x1_32\n",
    "        \n",
    "        pool = tf.nn.avg_pool(conv4_1x1_32, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "        #pool = tf.nn.avg_pool(conv6_1x1_64, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "        network['pool'] = pool\n",
    "        \n",
    "        dense = tf.reshape(pool, [-1, _w['dense1'].get_shape().as_list()[0]])\n",
    "        dense1 = tf.add(tf.matmul(dense, _w['dense1']), _b['dense1'])\n",
    "        dense1 = tf.nn.relu(dense1)\n",
    "        network['dense1'] = dense1\n",
    "        \n",
    "        logit = tf.add(tf.matmul(dense1, _w['dense2']), _b['dense2'])\n",
    "        network['logit'] = logit\n",
    "        \n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out = SimpleResNet(INPUT_SIDE, INPUT_SIDE, N_CHANNEL, x, weights, biases, 'SimpleResNet')\n",
    "print(\"CHECK LAYERS\")\n",
    "for key, value in out.items():\n",
    "    print (key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save\n",
    "saver = tf.train.Saver(max_to_keep=3)\n",
    "save_step = 100 #save for 100 epoch\n",
    "if not os.path.exists(CHECKPOINT_PATH):\n",
    "    os.makedirs(CHECKPOINT_PATH)\n",
    "print (\"SAVER READY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cost\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=out['logit']))\n",
    "tf.summary.scalar(\"loss\", cost)\n",
    "print(\"LOSS FUNCTION\")\n",
    "\n",
    "#learning rate\n",
    "global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "learning_rate = tf.train.exponential_decay(learning_rate=LR,\n",
    "                                           global_step=global_step,\n",
    "                                           decay_steps=50000,\n",
    "                                           decay_rate=LR_DECAY_RATE,\n",
    "                                           staircase=True,\n",
    "                                           name=\"learning_rate\")\n",
    "learning_rate = tf.maximum(learning_rate, 0.0001)\n",
    "tf.summary.scalar(\"learning_rate\", learning_rate)\n",
    "print(\"LERANING RATE : {}\".format(learning_rate))\n",
    "\n",
    "#optimizer\n",
    "adam = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "sgd = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "print(\"OPTIMIZER 1 : {}\".format(adam))\n",
    "print(\"OPTIMIZER 2 : {}\".format(sgd))\n",
    "\n",
    "corr = tf.equal(tf.argmax(out['logit'], 1), tf.argmax(y,1))\n",
    "accr = tf.reduce_mean(tf.cast(corr, \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "\n",
    "#summary_writer = tf.summary.FileWriter(CHECKPOINT_PATH, sess.graph)\n",
    "#checkpoint = tf.train.latesrt_checkpoint(CHECKPOINT_PATH)\n",
    "\n",
    "#포문돌다 세이브할땐\n",
    "#save_path = saver.save(sess, CHECKPOINT_PATH + \"mm.ckpt\", global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-1-95a999170a05>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-95a999170a05>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    a = tf.Variable(tf.random_normal([8, 1000], stddev=0.01 )\u001b[0m\n\u001b[0m                                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "a = tf.Variable(tf.random_normal([8196, 1000], stddev=0.01 ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
