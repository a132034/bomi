{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./cifar10/cifar-10-batches-py/\n",
      "input image size : 1024\n",
      "image channel : 3\n",
      "batch size : 128\n",
      "num of class : 10\n",
      "training epochs : 1000\n",
      "learning rate : 0.1\n",
      "learning decay rate : 0.5\n"
     ]
    }
   ],
   "source": [
    "#dir\n",
    "DATA_PATH = './cifar10/cifar-10-batches-py/' #pickle\n",
    "\n",
    "print(DATA_PATH)\n",
    "\n",
    "#param \n",
    "IMAGE_HEIGHT = 32\n",
    "IMAGE_WIDTH = 32\n",
    "IMAGE_SIZE = IMAGE_HEIGHT * IMAGE_WIDTH\n",
    "N_CHANNEL = 3\n",
    "LABEL_BYTE = 1\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "EPOCHS = 1000\n",
    "\n",
    "LEARNING_RATE = 0.1\n",
    "LR_DECAY_RATE = 0.5\n",
    "WEIGHT_DECAY = 0.0005\n",
    "MOVING_AVERAGE_DECAY = 0.9999\n",
    "\n",
    "NUM_EXAMPLES_PER_EPOCH = 50000 #NUMBER OF TRAIN DATA SET\n",
    "NUM_BATCHES_PER_EPOCH = NUM_EXAMPLES_PER_EPOCH / BATCH_SIZE\n",
    "NUM_EPOCHS_PER_DECAY = 350\n",
    "NUM_TESTDATA_PER_EPOCH = 10000\n",
    "NUM_TEST_BATCHES_PER_EPOCH = NUM_TESTDATA_PER_EPOCH / BATCH_SIZE\n",
    "\n",
    "LIST_CLASS=['airplane', 'automobile', 'birds', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "N_CLASSES = len(LIST_CLASS)\n",
    "\n",
    "print(\"input image size : {}\".format(IMAGE_SIZE))\n",
    "print(\"image channel : {}\".format(N_CHANNEL))\n",
    "print(\"batch size : {}\".format(BATCH_SIZE))\n",
    "print(\"num of class : {}\".format(N_CLASSES))\n",
    "print(\"training epochs : {}\".format(EPOCHS))\n",
    "print(\"learning rate : {}\".format(LEARNING_RATE))\n",
    "print(\"learning decay rate : {}\".format(LR_DECAY_RATE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA READ (CIFAR-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bytes_feature(value):\n",
    "    \"\"\"Wrapper for inserting bytes features into Example proto.\n",
    "    \"\"\"\n",
    "    if not isinstance(value, list):\n",
    "        value = [value]\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n",
    "\n",
    "def int64_feature(value):\n",
    "    \"\"\"Wrapper for inserting int64 features into Example proto.\n",
    "    \"\"\"\n",
    "    if not isinstance(value, list):\n",
    "        value = [value]\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [os.path.join(DATA_PATH, 'data_batch_%d' % i) for i in range(1, 6)]\n",
    "filenames.append(os.path.join(DATA_PATH, 'test_batch'))\n",
    "print(filenames)\n",
    "\n",
    "tf_filenames_train = [os.path.join(DATA_PATH, 'data_batch_%d.tfrecords' % i) for i in range(1, 6)]\n",
    "tf_filenames_test = [(os.path.join(DATA_PATH, 'test_batch.tfrecords'))]\n",
    "print(tf_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for path in filenames:\n",
    "    #tfrecord가 만들어진 경우 넘어감\n",
    "    tf_path = path + '.tfrecords'\n",
    "    if os.path.exists(tf_path):\n",
    "        continue\n",
    "    \n",
    "    #파일이 없는 경우 에러\n",
    "    if not os.path.exists(path):\n",
    "        raise ValueError('Failed to find file: ' + path)\n",
    "    \n",
    "    dict = unpickle(path)\n",
    "    writer = tf.python_io.TFRecordWriter(tf_path)\n",
    "    images = dict[b'data']\n",
    "    labels = dict[b'labels']\n",
    "    \n",
    "    for i in range(images.shape[0]):\n",
    "        image = images[i,:]\n",
    "        label = labels[i]\n",
    "        feature = {'cifar/image' : bytes_feature(value=[image.tostring()]), \n",
    "                   'cifar/label' : int64_feature(value=[label])}\n",
    "        example = tf.train.Example(features=tf.train.Features(features=feature))\n",
    "        serialized = example.SerializeToString()\n",
    "        writer.write(serialized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INPUT PIPELINE FROM TFRECORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def input_batches(num_threads=8, test=False, distorted=False):\n",
    "    min_fraction_of_examples_in_queue = 0.4\n",
    "    min_queue_examples = int(NUM_EXAMPLES_PER_EPOCH * min_fraction_of_examples_in_queue)\n",
    "    capacity = batch_size * 3 + min_queue_examples #queue capacity \n",
    "    \n",
    "    #test or train\n",
    "    tf_files = None\n",
    "    if test:\n",
    "        tf_files = tf_filenames_test\n",
    "    else:\n",
    "        tf_files = tf_filenames_train\n",
    "        \n",
    "    filename_queue = tf.train.string_input_producer(tf_files, shuffle=True)\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "    feature = {'cifar/image' : tf.FixedLenFeature(shape=[], dtype=tf.string),\n",
    "              'cifar/label' : tf.FixedLenFeature(shape=[], dtype=tf.int64)}\n",
    "    \n",
    "    features = tf.parse_single_example(serialized_example, features=feature)\n",
    "    \n",
    "    image = tf.reshape(tf.decode_raw(features['cifar/image'], tf.uint8), [IMAGE_HEIGHT, IMAGE_WIDTH, N_CHANNEL])\n",
    "    \n",
    "    image = tf.cast(image, tf.float32)\n",
    "    label = features['cifar/label']\n",
    "    \n",
    "    if distored:\n",
    "        image = tf.random_crop(image, [IMAGE_HEIGHT, IMAGE_WIDTH, N_CHANNEL]) #사실상 필요 없음\n",
    "        image = tf.image.random_flip_left_right(image)\n",
    "        image = tf.image.random_contrast(image, lower=0.2, upper=1.8)\n",
    "        #몇가지 더 추가해 볼 것.\n",
    "        \n",
    "        \n",
    "    #standardization \n",
    "    image = tf.image.per_image_standardization(image)\n",
    "    \n",
    "    image_batch, label_batch = tf.train.shuffle_batch([image, label], batch_size=BATCH_SIZE)\n",
    "    \n",
    "    return image_batch, label_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL ( RES NET )\n",
    "\n",
    "#### Residual 32 - 7x7 64 /2 > pool /2 > 3x3 64 > 3x3 64 > concat > 3x3 64 > 3x3 64 > concat > 3x3 64 > 3x3 64 > concat > 3x3 128 /2 > 3x3 128 > 3x3 128 > 3x3 128 > concat > 3x3 128 > 3x3 128 > concat > 3x3 128 > 3x3 128 > concat > 3x3 256 /2 > 3x3 256 > 3x3 256 >  3x3 256 > concat > 3x3 256 >  3x3 256 > concat > 3x3 256 >  3x3 256 > concat > 3x3 256 >  3x3 256 > concat > 3x3 256 >  3x3 256 > concat > 3x3 512 /2 > 3x3 512 > 3x3 512 > 3x3 512 > concat > 3x3 512 > 3x3 512 > concat > avg pool > fc 1000\n",
    "http://felixlaumon.github.io/2015/01/08/kaggle-right-whale.html\n",
    "\n",
    "\n",
    "#### cifar10-residual - 3x3 16 > 3x3 16 > 3x3 16 > concat > 3x316 > 3x3 16 > concat > max pool? > 3x3 32 > 3x3 32 > concat > 3x3 32 > 3x3 32 > concat > max pool? > 3 x 3 64 > 3 x 3 64 > concat  > 3 x 3 64 > 3 x 3 64 > concat >  avg pool > softmax\n",
    "http://laonple.blog.me/220770760226\n",
    "\n",
    "#### bottle neck > 1x1 out/4 > 3x3 out/4 > 1x1 > out\n",
    "\n",
    "## 참고해볼 것 http://florianmuellerklein.github.io/wRN_vs_pRN/\n",
    "## https://github.com/tensorflow/models/blob/master/resnet/resnet_model.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bottleneck(inputs, output_depth, is_training=False, reuse=False, scope=None):\n",
    "    with tf.variable_scope(scope, 'bottle_neck', reuse=reuse):\n",
    "        _output = tf.layers.conv2d(inputs, int(output_depth/4), (1,1), padding='SAME', \n",
    "                                   activation=tf.nn.relu, use_bias=False,\n",
    "                                   kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                   kernel_regularizer=tf.contrib.layers.l2_regularizer(WEIGHT_DECAY), \n",
    "                                   name='squeezed_conv1x1')\n",
    "        _output = tf.layers.conv2d(_output, int(output_depty/4), (3,3), padding='SAME', \n",
    "                                   activation=tf.nn.relu, use_bias=False,\n",
    "                                   kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                   kernel_regularizer=tf.contrib.layers.l2_regularizer(WEIGHT_DECAY), \n",
    "                                   name='squeezed_conv3x3')\n",
    "        _output = tf.layers.conv2d(_output, output_depty, (1,1), padding='SAME', \n",
    "                                   activation=tf.nn.relu, use_bias=False,\n",
    "                                   kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                   kernel_regularizer=tf.contrib.layers.l2_regularizer(WEIGHT_DECAY), \n",
    "                                   name='expanded_conv1x1')\n",
    "        _output = tf.layers.batch_normalization(_output, training=is_training, name='batch_norm_bottleneck')\n",
    "        return _output\n",
    "\n",
    "def resmodule(inputs, output_depth, is_training=False, reuse=False, scope=None):\n",
    "    with tf.variable_scope(scope, 'resnet_module', reuse=reuse):\n",
    "        _net = bottleneck(inputs, int(output_depth/2), is_training=is_training, reuse=reuse, name='bottleneck1')\n",
    "        print(net.name, net.get_shape())\n",
    "        _net = bottleneck(_net, int(output_depth/2), is_training=is_training, reuse=reuse, name='bottleneck2')\n",
    "        print(net.name, net.get_shape())\n",
    "        _output = tf.concat([net, inputs], axis=3)\n",
    "        return _output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RES(images, num_classes, is_training=False, reuse=False, scope='ResNet'):\n",
    "    with tf.variable_scope(scope, 'resnet', reuse=reuse):\n",
    "        net = tf.layers.conv2d(images, 16, (3,3), padding='SAME',\n",
    "                               activation=tf.nn.relu, use_bias=False,\n",
    "                               kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                               kernel_regularizer=tf.contrib.layers.l2_regularizer(WEIGHT_DECAY), \n",
    "                               name='1st_conv')\n",
    "        print(net.name, net.get_shape())\n",
    "        net = resmodule(net, 32, is_training=is_training, reuse=reuse)\n",
    "        print(net.name, net.get_shape())\n",
    "        net = tf.layers.max_pooling2d(net, (2,2), strides=(2,2), padding='SAME', name='maxpool1')\n",
    "        print(net.name, net.get_shape())\n",
    "        net = resmodule(net, 64, is_training=is_training, reuse=reuse)\n",
    "        print(net.name, net.get_shape())\n",
    "        net = tf.layers.max_pooling2d(net, (2,2), strides=(2,2), padding='SAME', name='maxpool2')\n",
    "        print(net.name, net.get_shape())\n",
    "        net = resmodule(net, 128, is_training=is_training, reuse=reuse)\n",
    "        print(net.name, net.get_shape())\n",
    "        net = tf.layers.average_pooling2d(net, (2,2), strides=(2,2), padding='SAME', name='avgpool')\n",
    "        print(net.name, net.get_shape())\n",
    "        net = tf.layers.conv2d(net, num_classes, (1,1), name='flatten')\n",
    "        print(net.name, net.get_shape())\n",
    "        logits = tf.squeeze(net, (1,2), name='logits')\n",
    "        print(logits.name, logits.get_shape())\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL ( VGG NET )\n",
    "\n",
    "#### VGG 19 - 3x3 64 > 3x3 64 > pool /2 > 3x3 128 >  3x3 128 > pool /2 >  3x3 256 > 3x3 256 > 3x3 256 > 3x3 256 > pool /2 > 3x3 512 > 3x3 512 > 3x3 512 > 3x3 512 > pool /2 > 3x3 512 > 3x3 512 > 3x3 512 > 3x3 512 > pool /2 > fc 4096 > fc 4096 > fc 1000 > classes\n",
    "#### relu >>>>> softmax\n",
    "\n",
    "#### VGG 16 3x3 64 > 3x3 64 > pool /2 > 3x3 128 >  3x3 128 > pool /2 > 3x3 256 > 3x3 256 > 3x3 256 > pool /2 > 3x3 512 > 3x3 512 > 3x3 512 > pool /2 > 3x3 512 > 3x3 512 > 3x3 512 > pool /2 > fc 4096 > fc 4096 > fc 1000 > classes\n",
    "#### relu >>>>> softmax\n",
    "\n",
    "#### VGG for CIFAR-10 > 3x3 16 > 3x3 16 > pool /2 > 3x3 32 >  3x3 32 > 3x3 32 > pool /2 > 3x3 64 > 3x3 64 > 3x3 64 > 3x3 64 > pool /2 > fc 2048 > fc 2048 >  fc 500 > softmax\n",
    "#### relu >>>>> softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def VGG(images, reuse=False):\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOSS, TRAIN(EVAL) OP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(logits, labels, total=True):\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits, name='cross_entropy')\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy, name='mean_cross_entropy')\n",
    "    tf.add_to_collection('losses', cross_entropy_mean) #loss +++ \n",
    "    _loss = cross_entropy_mean\n",
    "    \n",
    "    if total:\n",
    "        print(\"adding total loss\", len(tf.get_collection('losses')))\n",
    "        _loss = tf.add_n(tf.get_collection('losses'), name='total_loss')\n",
    "    return _loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _add_total_loss_summary(total_loss, test=False):\n",
    "    emaloss = tf.train.ExponentialMovingAverage(decay=0.9, name='avg')\n",
    "    losses = []\n",
    "    if not test:\n",
    "        losses = tf.get_collection('losses')\n",
    "\n",
    "    loss_average_op = emaloss.apply(losses + [total_loss])\n",
    "    for ls in losses + [total_loss]:\n",
    "        tf.summary.scalar(ls.op.name + '_raw_', ls)\n",
    "        tf.summary.scalar(ls.op.name + 'ema', emaloss.average(ls))\n",
    "    return loss_average_op\n",
    "\n",
    "#http://cafe.naver.com/soynature/1942\n",
    "#ema / bn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainop(global_step, topk=1):\n",
    "    decay_steps = int(NUM_BATCHES_PER_EPOCH * NUM_EPOCHS_PER_DECAY)\n",
    "    lr = tf.train.exponential_decay(LEARNING_RATE, global_step, decay_steps, LR_DECAY_RATE, staircase=True)\n",
    "    tf.summary.scalar('learning_rate', lr)\n",
    "    \n",
    "    images, labels = input_batches(num_threads=8)\n",
    "    logits = RES(images=images, is_training=True, num_classes=N_CLASSES)\n",
    "    totalloss = loss(logits, labels)\n",
    "    predictions = tf.nn.in_top_k(logits, labels, topk)\n",
    "    truecount = tf.cast(predictions, tf.float32)\n",
    "    accuracy = tf.reduce_mean(truecount)\n",
    "   \n",
    "    loss_average_op = _add_total_loss_summary(totalloss)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "    with tf.control_dependencies([loss_average_op]): #gradient clip ... 등등을 위해서 보통 나눔\n",
    "        gradsvars = optimizer.compute_gradients(totalloss)\n",
    "        apply_grads_op = optimizer.apply_gradients(gradsvars, global_step=global_step)\n",
    "    for grad, var in gradsvars:\n",
    "        tf.summary.histogram(var.op.name, var)\n",
    "        if grad is not None:\n",
    "            tf.summary.histogram(var.op.name + '/grads', grad)\n",
    "    \n",
    "    ema_variables = tf.train.ExponentialMovingAverage(decay=MOVING_AVERAGE_DECAY, global_step)\n",
    "    vars_averages_op = ema_variables.apply(tf.trainable_variables())\n",
    "\n",
    "    with tf.control_dependencies([apply_grads_op, vars_averages_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "    \n",
    "    return train_op, totalloss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evalop(topk=1):\n",
    "    images, labels = input_batches(2, test=True)\n",
    "    logits = RES(images=images, is_training=False, num_classes=N_CLASSES, reuse=True)\n",
    "    evalloss = loss(logits, labels, total=False)\n",
    "    predictions = tf.nn.in_top_k(logits, labels, topk)\n",
    "    truecount = tf.cast(predictions, tf.float32)\n",
    "    accuracy = tf.reduce_mean(truecount)\n",
    "    return evalloss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    with tf.Graph().as_default():\n",
    "        global_step = tf.contrib.framework.get_or_create_global_step() #global step\n",
    "        train_op, totalloss, trainaccuracy = trainop(global_step) #train op\n",
    "        evalloss, evalaccuracy = evalop() #eval op\n",
    "        class _LoggerHook(tf.train.SessionRunHook): #for문이랑 비슷한 역할임. > 세션훅 상속\n",
    "            def begin(self): #가장 처음 한번만 실행 \n",
    "                self._step = -1\n",
    "            def before_run(self, run_context): #포문 \n",
    "                self._step += 1\n",
    "                self._start_time = time.time()\n",
    "                if self._step % 100 == 0:\n",
    "                    return tf.train.SessionRunArgs([totalloss, trainaccuracy, evalloss, evalaccuracy])\n",
    "                if self._step % 10 == 0:\n",
    "                    return tf.train.SessionRunArgs([totalloss, trainaccuracy])\n",
    "                else:\n",
    "                    return tf.train.SessionRunArgs([totalloss])\n",
    "            def after_run(self, run_context, run_values): #끝? 리턴 후에만 실행??? \n",
    "                duration = time.time() - self._start_time\n",
    "                loss_value = run_values.results[0]\n",
    "                if self._step % 10 == 0:\n",
    "                    trainaccuracy = run_values.results[1]\n",
    "                    num_examples_per_step = Flags.batch_size\n",
    "                    examples_per_sec = num_examples_per_step / duration\n",
    "                    sec_per_batch = float(duration)\n",
    "\n",
    "                    format_str = ('%s: step %d, loss = %.2f  accuracy = %.2f (%.1f examples/sec; %.3f '\n",
    "                                  'sec/batch)')\n",
    "                    print (format_str % (time.time(), self._step, loss_value, trainaccuracy, \n",
    "                               examples_per_sec, sec_per_batch))\n",
    "                if self._step % 100 == 0:\n",
    "                    evalloss = run_values.results[2]\n",
    "                    accuracy = run_values.results[3]\n",
    "                    print('eval result: eval_loss: %.2f  accuracy: %.2f'  % (evalloss, accuracy))\n",
    "                    print('')\n",
    "            \n",
    "        with tf.train.MonitoredTrainingSession( #모니터링세션\n",
    "            checkpoint_dir=Flags.train_dir, \n",
    "            hooks=[tf.train.StopAtStepHook(last_step=Flags.max_steps), tf.train.NanTensorHook(totalloss), _LoggerHook()], #훅들\n",
    "            config=tf.ConfigProto(log_device_placement=Flags.log_device_placement)) as mon_sess:\n",
    "            while not mon_sess.should_stop():\n",
    "                mon_sess.run(train_op)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
