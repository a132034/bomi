{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import OrderedDict #순서까지 기억하는 사전형\n",
    "import os, random\n",
    "import pickle\n",
    "# cifar 10 \n",
    "#wget http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
    "#tar xzf cifar-10-python.tar.gz\n",
    "# resnet\n",
    "#http://laonple.blog.me/220761052425\n",
    "#http://laonple.blog.me/220770760226"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 파라미터들\n",
    "input_size = 32\n",
    "data_path = 'cifar10/cifar-10-batches-py/'\n",
    "\n",
    "#train&test batch\n",
    "train_file=['data_batch_1','data_batch_2', 'data_batch_3', 'data_batch_4', 'data_batch_5']\n",
    "test_file=['cifar10/cifar-10-batches-py/test_batch']\n",
    "\n",
    "\n",
    "list_class=['airplane', 'automobile', 'birds', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "n_side = 32\n",
    "n_size = n_side * n_side\n",
    "n_channel = 3\n",
    "\n",
    "n_class = len(list_class)\n",
    "n_layer = 3 #simple resnet\n",
    "\n",
    "n_batch = 100\n",
    "\n",
    "#input & output \n",
    "x = tf.placeholder(\"float\", [n_batch, n_size * n_channel]) #batch x image size\n",
    "y = tf.placeholder(\"float\", [n_batch, n_class]) #batch x class\n",
    "\n",
    "#weight&bias\n",
    "stddev = 0.1\n",
    "weights = {\n",
    "    'c': tf.Variable(tf.random_normal([3, 3, n_channel, 16], stddev=stddev)),\n",
    "    'c1_1': tf.Variable(tf.random_normal([3, 3, 16, 16], stddev=stddev)),\n",
    "    'c1_2': tf.Variable(tf.random_normal([3, 3, 16, 16], stddev=stddev)),\n",
    "    'c1_3': tf.Variable(tf.random_normal([3, 3, 16, 16], stddev=stddev)),\n",
    "    'c1_4': tf.Variable(tf.random_normal([3, 3, 16, 16], stddev=stddev)),\n",
    "    'c2_1': tf.Variable(tf.random_normal([3, 3, 16, 32], stddev=stddev)),\n",
    "    'c2_2': tf.Variable(tf.random_normal([3, 3, 32, 32], stddev=stddev)),\n",
    "    'c2_3': tf.Variable(tf.random_normal([3, 3, 32, 32], stddev=stddev)),\n",
    "    'c2_4': tf.Variable(tf.random_normal([3, 3, 32, 32], stddev=stddev)),\n",
    "    'c3_1': tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=stddev)),\n",
    "    'c3_2': tf.Variable(tf.random_normal([3, 3, 64, 64], stddev=stddev)),\n",
    "    'c3_3': tf.Variable(tf.random_normal([3, 3, 64, 64], stddev=stddev)),\n",
    "    'c3_4': tf.Variable(tf.random_normal([3, 3, 64, 64], stddev=stddev)),\n",
    "    'd1' :tf.Variable(tf.random_normal([16*16*64, 1000], stddev=stddev)),\n",
    "    'd2' :tf.Variable(tf.random_normal([1000, n_class], stddev=stddev))\n",
    "}\n",
    "biases = {\n",
    "    'c': tf.Variable(tf.random_normal([16], stddev=stddev)), \n",
    "    'c1_1': tf.Variable(tf.random_normal([16], stddev=stddev)),\n",
    "    'c1_2': tf.Variable(tf.random_normal([16], stddev=stddev)),\n",
    "    'c1_3': tf.Variable(tf.random_normal([16], stddev=stddev)),\n",
    "    'c1_4': tf.Variable(tf.random_normal([16], stddev=stddev)),\n",
    "    'c2_1': tf.Variable(tf.random_normal([32], stddev=stddev)),\n",
    "    'c2_2': tf.Variable(tf.random_normal([32], stddev=stddev)),\n",
    "    'c2_3': tf.Variable(tf.random_normal([32], stddev=stddev)),\n",
    "    'c2_4': tf.Variable(tf.random_normal([32], stddev=stddev)),\n",
    "    'c3_1': tf.Variable(tf.random_normal([64], stddev=stddev)),\n",
    "    'c3_2': tf.Variable(tf.random_normal([64], stddev=stddev)),\n",
    "    'c3_3': tf.Variable(tf.random_normal([64], stddev=stddev)),\n",
    "    'c3_4': tf.Variable(tf.random_normal([64], stddev=stddev)),\n",
    "    'd1': tf.Variable(tf.random_normal([1000], stddev=stddev)),\n",
    "    'd2': tf.Variable(tf.random_normal([n_class], stddev=stddev))\n",
    "}\n",
    "\n",
    "#시작은 맥스풀\n",
    "#끝은 에버리지 풀 \n",
    "# 3x3 conv 2개마다 2개 이전의 결과를 더해줌 \n",
    "#마지막은 Fc1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model\n",
    "def ResNet(img_width, img_height, img_channel, _x, _w, _b, scope='Simple_ResNet'):\n",
    "    _names = OrderedDict()\n",
    "    print('img_width')\n",
    "    print(img_width)\n",
    "    print('img_height')\n",
    "    print(img_height)\n",
    "    print('img_channel')\n",
    "    print(img_channel)\n",
    "    print(_x.shape)\n",
    "\n",
    "    #reshape\n",
    "    _x_r = tf.reshape(_x, shape=[-1, img_width, img_height, img_channel])\n",
    "    \n",
    "    with tf.variable_scope(scope):\n",
    "        #max pool\n",
    "        _pool = tf.nn.max_pool(_x_r, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "        _names['pool'] = _pool\n",
    "        \n",
    "        #convolution\n",
    "        _conv = tf.nn.conv2d(_pool, _w['c'], strides=[1,1,1,1], padding='SAME')\n",
    "        _conv = tf.nn.bias_add(_conv, _b['c'])\n",
    "        _conv = tf.nn.relu(_conv)\n",
    "        _names['conv'] = _conv\n",
    "        \n",
    "        #residual conv - 1\n",
    "        conv1_1 = tf.nn.conv2d(_conv, _w['c1_1'], strides=[1,1,1,1], padding='SAME')\n",
    "        conv1_1 = tf.nn.bias_add(conv1_1, _b['c1_1'])\n",
    "        conv1_1 = tf.nn.relu(conv1_1)\n",
    "        _names['conv1_1'] = conv1_1\n",
    "        \n",
    "        conv1_2 = tf.nn.conv2d(conv1_1, _w['c1_2'], strides=[1,1,1,1], padding='SAME')\n",
    "        conv1_2 = tf.nn.bias_add(conv1_2, _b['c1_2'])\n",
    "        conv1_2 = tf.add(conv1_2 , _conv) #residual? \n",
    "        conv1_2 = tf.nn.relu(conv1_2)\n",
    "        _names['conv1_2'] = conv1_2\n",
    "        \n",
    "        conv1_3 = tf.nn.conv2d(conv1_2, _w['c1_3'], strides=[1,1,1,1], padding='SAME')\n",
    "        conv1_3 = tf.nn.bias_add(conv1_3, _b['c1_3'])\n",
    "        conv1_3 = tf.nn.relu(conv1_3)\n",
    "        _names['conv1_3'] = conv1_3\n",
    "        \n",
    "        conv1_4 = tf.nn.conv2d(conv1_3, _w['c1_4'], strides=[1,1,1,1], padding='SAME')\n",
    "        conv1_4 = tf.nn.bias_add(conv1_4, _b['c1_4'])\n",
    "        conv1_4 = tf.add(conv1_4 , conv1_2) #residual? \n",
    "        conv1_4 = tf.nn.relu(conv1_4)\n",
    "        _names['conv1_4'] = conv1_4\n",
    "        \n",
    "        #residual conv - 2\n",
    "        conv2_1 = tf.nn.conv2d(conv1_4, _w['c2_1'], strides=[1,1,1,1], padding='SAME')\n",
    "        conv2_1 = tf.nn.bias_add(conv2_1, _b['c2_1'])\n",
    "        conv2_1 = tf.nn.relu(conv2_1)\n",
    "        _names['conv2_1'] = conv2_1\n",
    "        \n",
    "        conv2_2 = tf.nn.conv2d(conv2_1, _w['c2_2'], strides=[1,1,1,1], padding='SAME')\n",
    "        conv2_2 = tf.nn.bias_add(conv2_2, _b['c2_2'])\n",
    "        #conv2_2 = tf.add(conv2_2, conv1_4)\n",
    "        conv2_2 = tf.add(conv2_2, conv2_1)\n",
    "        conv2_2 = tf.nn.relu(conv2_2)\n",
    "        _names['conv2_2'] = conv2_2\n",
    "        \n",
    "        conv2_3 = tf.nn.conv2d(conv2_2, _w['c2_3'], strides=[1,1,1,1], padding='SAME')\n",
    "        conv2_3 = tf.nn.bias_add(conv2_3, _b['c2_3'])\n",
    "        conv2_3 = tf.nn.relu(conv2_3)\n",
    "        _names['conv2_3'] = conv2_3\n",
    "        \n",
    "        conv2_4 = tf.nn.conv2d(conv2_3, _w['c2_4'], strides=[1,1,1,1], padding='SAME')\n",
    "        conv2_4 = tf.nn.bias_add(conv2_4, _b['c2_4'])\n",
    "        conv2_4 = tf.add(conv2_4, conv2_2)\n",
    "        conv2_4 = tf.nn.relu(conv2_4)\n",
    "        _names['conv2_4'] = conv2_4\n",
    "        \n",
    "        #residual conv - 3\n",
    "        conv3_1 = tf.nn.conv2d(conv2_4, _w['c3_1'], strides=[1,1,1,1], padding='SAME')\n",
    "        conv3_1 = tf.nn.bias_add(conv3_1, _b['c3_1'])\n",
    "        conv3_1 = tf.nn.relu(conv3_1)\n",
    "        _names['conv3_1'] = conv3_1\n",
    "        \n",
    "        conv3_2 = tf.nn.conv2d(conv3_1, _w['c3_2'], strides=[1,1,1,1], padding='SAME')\n",
    "        conv3_2 = tf.nn.bias_add(conv3_2, _b['c3_2'])\n",
    "        #conv3_2 = tf.add(conv3_2, conv2_4)\n",
    "        conv3_2 = tf.add(conv3_2, conv3_1)\n",
    "        conv3_2 = tf.nn.relu(conv3_2)\n",
    "        _names['conv3_2'] = conv3_2\n",
    "        \n",
    "        conv3_3 = tf.nn.conv2d(conv3_2, _w['c3_3'], strides=[1,1,1,1], padding='SAME')\n",
    "        conv3_3 = tf.nn.bias_add(conv3_3, _b['c3_3'])\n",
    "        conv3_2 = tf.nn.relu(conv3_3)\n",
    "        _names['conv3_3'] = conv3_3\n",
    "        \n",
    "        conv3_4 = tf.nn.conv2d(conv3_3, _w['c3_4'], strides=[1,1,1,1], padding='SAME')\n",
    "        conv3_4 = tf.nn.bias_add(conv3_4, _b['c3_4'])\n",
    "        conv3_4 = tf.add(conv3_4, conv3_2)\n",
    "        conv3_4 = tf.nn.relu(conv3_4)\n",
    "        _names['conv3_4'] = conv3_4\n",
    "        \n",
    "        \n",
    "        \n",
    "        #vectorize\n",
    "        _dense = tf.reshape(conv3_4, [-1, _w['d1'].get_shape().as_list()[0]])\n",
    "        _names['dense'] = _dense\n",
    "        \n",
    "        d1 = tf.add(tf.matmul(_dense, _w['d1']), _b['d1'])\n",
    "        _names['d1'] = d1\n",
    "        print(d1.shape)\n",
    "        \n",
    "        _logit = tf.add(tf.matmul(d1, _w['d2']), _b['d2'])\n",
    "        _names['logit'] = _logit\n",
    "        print(_logit)\n",
    "                \n",
    "    return _names        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#prediction\n",
    "out = ResNet(n_side, n_side, n_channel, x, weights, biases, 'Simple_ResNet')\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cost function & loss\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=out['logit']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#optimizer\n",
    "optm = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "corr = tf.equal(tf.argmax(out['logit'], 1), tf.argmax(y,1))\n",
    "accr = tf.reduce_mean(tf.cast(corr, \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializer\n",
    "init = tf.global_variables_initializer()\n",
    "print (\"FUNCTIONS READY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out['dense'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save\n",
    "savedir = \"nets/cnn_cifar10_resnet/\"\n",
    "saver = tf.train.Saver(max_to_keep=3)\n",
    "save_step = 4\n",
    "if not os.path.exists(savedir):\n",
    " os.makedirs(savedir)\n",
    "print (\"SAVER READY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_npz(file):\n",
    " \n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "        return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Make batch_iterator for npz data\n",
    "def make_batch_index(_data, batch_size=128, allow_small_batch=True):\n",
    "    num_images = len(_data[b'data'])\n",
    "    start_idx = list(range(0, num_images, batch_size))\n",
    "    end_idx = list(range(batch_size, num_images + 1, batch_size))\n",
    "    if allow_small_batch and end_idx[-1] < num_images :\n",
    "        start_idx.append(end_idx[-1])\n",
    "        end_idx.append(num_images)\n",
    "    return zip(start_idx, end_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(x, numclass):\n",
    "    return np.eye(numclass)[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_epochs = 1000\n",
    "batch_size = 100\n",
    "disp_each = 10\n",
    "disp_batch =100\n",
    "# LAUNCH THE GRAPH\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_testdata=load_npz(test_file[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "for epoch in range(training_epochs):\n",
    "    \n",
    "    sample_list_new = random.sample(train_file, len(train_file))\n",
    "    nNumBatch = 0\n",
    "    AvgBatchCost = 0\n",
    "    \n",
    "    for index, file_name in enumerate(sample_list_new):\n",
    "        file_name = data_path+file_name\n",
    "        load_data = load_npz(file_name)\n",
    "        batch_index = make_batch_index(load_data, batch_size, True)\n",
    "        \n",
    "        for start, end in batch_index:\n",
    "            print(str(start)+\", \"+str(end))\n",
    "            batch_xs = load_data[b'data'][start:end]\n",
    "            batch_ys = one_hot_encode(load_data[b'labels'][start:end], n_class)\n",
    "            #print(batch_ys)\n",
    "            #print(batch_xs, shape)\n",
    "            feeds = {x: batch_xs, y: batch_ys}\n",
    "            sess.run(optm, feed_dict = feeds)\n",
    "            nNumBatch+=1\n",
    "            tmp_cost, _ = sess.run([cost, optm], feed_dict = feeds)\n",
    "            AvgBatchCost += tmp_cost\n",
    "        \n",
    "        if nNumBatch % disp_batch == 0:\n",
    "            train_acc = sess.run(accr, feed_dict = feeds)\n",
    "            print('\\t[%d nNumBatch] train_cost = %g, acc = %g'%(nNumBatch, AvgBatchCost/nNumBatch, train_acc))\n",
    "        print(nNumBatch)\n",
    "             \n",
    "    if epoch % disp_each == 0:\n",
    "        num_batch = 0\n",
    "        test_acc = 0\n",
    "        avg_acc = 0\n",
    "        load_test_data = load_npz(test_file[0])\n",
    "        testdatalabel = one_hot_encode(load_testdata[b'labels'], n_class)\n",
    "        test_batch_index = make_batch_index(load_data, batch_size, True)\n",
    "                \n",
    "        for start, end in test_batch_index:\n",
    "            num_batch += 1\n",
    "            batch_xs = load_testdata[b'data'][start:end]\n",
    "            batch_ys = one_hot_encode(load_testdata[b'labels'][start:end],n_class)\n",
    "            test_acc = sess.run(accr, feed_dict={x:batch_xs,y:batch_ys})\n",
    "            avg_acc+=test_acc\n",
    "        print('[%d epoch]test acc=%g'%(epoch, avg_acc/num_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_batch_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for start, end in test_batch_index:\n",
    "    print(start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
